{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea3706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 21:02:43.233068: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.9.86). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from brax.envs import State as EnvState\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Tuple, Optional, List\n",
    "\n",
    "from qdax import environments\n",
    "\n",
    "from qdax.core.neuroevolution.buffers.buffer import (\n",
    "    ReplayBuffer,\n",
    "    Transition,\n",
    ")\n",
    "\n",
    "from qdax.core.neuroevolution.buffers.buffer import ReplayBuffer, Transition\n",
    "from qdax.core.neuroevolution.networks.matd3_networks import make_matd3_networks\n",
    "from qdax.core.neuroevolution.mdp_utils import TrainingState\n",
    "from qdax.custom_types import Metrics\n",
    "from qdax.custom_types import (\n",
    "    Action,\n",
    "    Descriptor,\n",
    "    Mask,\n",
    "    Metrics,\n",
    "    Observation,\n",
    "    Params,\n",
    "    Reward,\n",
    "    RNGKey,\n",
    ")\n",
    "\n",
    "# Multiagent shiet\n",
    "from qdax.environments.multi_agent_wrappers import MultiAgentBraxWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8598672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "num_iterations = 1000\n",
    "log_interval = 10\n",
    "env_name = 'hopper_uni'#@param['ant_uni', 'hopper_uni', 'walker_uni', 'halfcheetah_uni', 'humanoid_uni', 'ant_omni', 'humanoid_omni']\n",
    "parameter_sharing=False\n",
    "emitter_type=\"mix\"\n",
    "homogenisation_method=\"concat\"\n",
    "episode_length = 1000 #@param {type:\"integer\"}\n",
    "num_timesteps = 7_864_320 #@param {type:\"integer\"}\n",
    "seed = 1 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (64, 64) #@param {type:\"raw\"}\n",
    "policy_learning_rate = 3e-4\n",
    "min_bd = 0. #@param {type:\"number\"}\n",
    "max_bd = 1.0 #@param {type:\"number\"}\n",
    "warmstart_steps=25_600\n",
    "num_evals=20\n",
    "\n",
    "# CEM\n",
    "warmup_iters: int = 10 # number of iter update with only CEM\n",
    "population_size: int = 10\n",
    "num_best: Optional[int] = None\n",
    "damp_init: float = 1e-3\n",
    "damp_final: float = 1e-5\n",
    "damp_tau : float = 0.95\n",
    "rank_weight_shift: float = 1.0\n",
    "mirror_sampling: bool = True\n",
    "weighted_update: bool = True\n",
    "num_learning_offspring: Optional[int] = population_size//2\n",
    "# TD3 params\n",
    "num_rl_updates_per_iter: int = 4000\n",
    "env_batch_size = 128 #@param {type:\"number\"}\n",
    "batch_size=256\n",
    "expl_noise = 0.1\n",
    "policy_noise = 0.2\n",
    "noise_clip = 0.5\n",
    "grad_updates_per_step=1.0 #@param {type:\"number\"}\n",
    "replay_buffer_size = 1000000 #@param {type:\"number\"}\n",
    "critic_hidden_layer_sizes = (256, 256) #@param {type:\"raw\"}\n",
    "critic_learning_rate = 1e-3\n",
    "discount = 0.99 #@param {type:\"number\"}\n",
    "reward_scaling = 1.0 #@param {type:\"number\"}\n",
    "soft_tau_update = 0.005 #@param {type:\"number\"}\n",
    "policy_delay = 4 #@param {type:\"number\"}\n",
    "max_grad_norm = 30.0\n",
    "use_layer_norm=True\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2520be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdax.baselines.cem_matd3 import CEMMATD3, CEMMATD3Config, CEMMATD3TrainingState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832d82c",
   "metadata": {},
   "source": [
    "## Warmstart related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4558126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Define the fonction random\n",
    "def warmstart_play_step_fn(\n",
    "    env_state: EnvState,\n",
    "    random_key: RNGKey,\n",
    "    env: MultiAgentBraxWrapper,\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "\n",
    "    action_sizes = env.get_action_sizes()\n",
    "\n",
    "    keys = jax.random.split(subkey, len(action_sizes))\n",
    "\n",
    "    actions = {\n",
    "        agent_idx: jax.random.uniform(agent_key, (size,), minval=-1, maxval=1)\n",
    "        for (agent_idx, size), agent_key in zip(action_sizes.items(), keys)\n",
    "    }\n",
    "\n",
    "    flatten_actions = jnp.concatenate([a for a in actions.values()])\n",
    "\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = Transition(\n",
    "        obs=next_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        actions=flatten_actions,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "    )\n",
    "\n",
    "    return next_state, random_key, transition\n",
    "\n",
    "def generate_unroll_warmstart(\n",
    "    random_key: RNGKey,\n",
    "    env_state: EnvState,\n",
    "    env: MultiAgentBraxWrapper,\n",
    "    warmstart_play_step_fn: Callable[\n",
    "        [EnvState, RNGKey, MultiAgentBraxWrapper],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    "    warmstart_steps: int,\n",
    ") -> Tuple[EnvState, Transition]:\n",
    "    \"\"\"Pre-populates the buffer with transitions. Returns the warmstarted buffer\n",
    "    and the new state of the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def _scan_play_step_fn(\n",
    "        carry: Tuple[EnvState, RNGKey], unused_arg: Any\n",
    "    ) -> Tuple[Tuple[EnvState, RNGKey], Transition]:\n",
    "        env_state, random_key, transitions = warmstart_play_step_fn(*carry, env)\n",
    "        return (env_state, random_key), transitions\n",
    "\n",
    "    (env_state, random_key), transitions = jax.lax.scan(\n",
    "        _scan_play_step_fn,\n",
    "        (env_state, random_key),\n",
    "        (),\n",
    "        length=warmstart_steps\n",
    "    )\n",
    "\n",
    "    return env_state, transitions\n",
    "\n",
    "@functools.partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"env\", \"warmstart_play_step_fn\", \"warmstart_steps\", \"env_batch_size\")\n",
    ")\n",
    "def warmstart_buffer(\n",
    "    env: MultiAgentBraxWrapper,\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    training_state: CEMMATD3TrainingState,\n",
    "    warmstart_play_step_fn: Callable[\n",
    "        [EnvState, RNGKey, MultiAgentBraxWrapper],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    "    warmstart_steps: int,\n",
    "    env_batch_size: int,\n",
    "):\n",
    "    \n",
    "\n",
    "    generate_unroll = functools.partial(\n",
    "        generate_unroll_warmstart,\n",
    "        env = env,\n",
    "        warmstart_play_step_fn=warmstart_play_step_fn,\n",
    "        warmstart_steps=warmstart_steps//env_batch_size\n",
    "    )\n",
    "\n",
    "    generate_unroll_vmap = jax.vmap(\n",
    "        generate_unroll,\n",
    "        in_axes=(0, 0)\n",
    "    )\n",
    "\n",
    "    random_key, subkey = jax.random.split(training_state.random_key)\n",
    "    keys = jax.random.split(subkey, env_batch_size)\n",
    "\n",
    "\n",
    "    training_state = training_state.replace(\n",
    "        random_key=random_key\n",
    "    )\n",
    "\n",
    "    reset_fn = jax.vmap(env.reset)\n",
    "\n",
    "    env_states = reset_fn(keys)\n",
    "\n",
    "    random_key, subkey = jax.random.split(training_state.random_key)\n",
    "\n",
    "    training_state = training_state.replace(\n",
    "        random_key=random_key\n",
    "    )\n",
    "\n",
    "    keys = jax.random.split(subkey, env_batch_size)\n",
    "    env_states, transitions = generate_unroll_vmap(keys, env_states)\n",
    "\n",
    "    # jax.debug.print(\"obs shape {obs}\", obs=transitions.obs.shape)\n",
    "\n",
    "    replay_buffer = replay_buffer.insert(transitions)\n",
    "    \n",
    "    return replay_buffer, training_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1efe1f",
   "metadata": {},
   "source": [
    "## Prepare env and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c6605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1, 2: 1}\n",
      "{0: 8, 1: 9, 2: 8}\n"
     ]
    }
   ],
   "source": [
    "base_env_name = env_name.split(\"_\")[0]\n",
    "env = environments.create(env_name, episode_length=episode_length)\n",
    "env = MultiAgentBraxWrapper(\n",
    "    env,\n",
    "    env_name=base_env_name,\n",
    "    parameter_sharing=False,\n",
    "    emitter_type=emitter_type,\n",
    "    homogenisation_method=homogenisation_method\n",
    ")\n",
    "\n",
    "policy_network, critic_network = make_matd3_networks(\n",
    "    action_sizes=env.get_action_sizes(),\n",
    "    critic_hidden_layer_sizes=critic_hidden_layer_sizes,\n",
    "    policy_hidden_layer_sizes=policy_hidden_layer_sizes,\n",
    "    use_layer_norm=use_layer_norm,\n",
    ")\n",
    "\n",
    "def play_step_fn(\n",
    "    env_state: EnvState,\n",
    "    policy_params: List[Params],\n",
    "    random_key: RNGKey,\n",
    ") -> Tuple[EnvState, RNGKey, Transition]:\n",
    "    \"\"\"Plays a step in the environment. Selects an action according to TD3 rule and\n",
    "    performs the environment step.\n",
    "\n",
    "    Args:\n",
    "        env_state: the current environment state\n",
    "        training_state: the SAC training state\n",
    "        env: the environment\n",
    "        deterministic: whether to select action in a deterministic way.\n",
    "            Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        the new environment state\n",
    "        the new TD3 training state\n",
    "        the played transition\n",
    "    \"\"\"\n",
    "    obs=env.obs(env_state)\n",
    "    actions = {\n",
    "        agent_idx: network.apply(params, agent_obs)\n",
    "        for (agent_idx, network), params, agent_obs in zip(\n",
    "            policy_network.items(), policy_params, obs.values()\n",
    "        )\n",
    "    }\n",
    "\n",
    "    next_env_state = env.step(env_state, actions)\n",
    "\n",
    "    flatten_action = jnp.concatenate([a for a in actions.values()])\n",
    "\n",
    "    transition = Transition(\n",
    "        obs=env_state.obs,\n",
    "        next_obs=next_env_state.obs,\n",
    "        rewards=next_env_state.reward,\n",
    "        dones=next_env_state.done,\n",
    "        truncations=next_env_state.info[\"truncation\"],\n",
    "        actions=flatten_action,\n",
    "    )\n",
    "    return next_env_state, random_key, transition\n",
    "\n",
    "\n",
    "\n",
    "def generate_unroll(\n",
    "        policy_params,\n",
    "        env_state,\n",
    "        random_key,\n",
    "        play_step_fn,\n",
    "        episode_length,\n",
    "):\n",
    "    def _scan_play_step_fn(\n",
    "        carry: Tuple[EnvState, Params, RNGKey], unused_arg: Any\n",
    "    ):\n",
    "        env_state, policy_params, random_key = carry\n",
    "        next_state, random_key, transition = play_step_fn(\n",
    "            env_state, policy_params, random_key\n",
    "        )\n",
    "        return (next_state, policy_params, random_key), transition\n",
    "\n",
    "    (env_state, policy_params, random_key), transitions = jax.lax.scan(\n",
    "        _scan_play_step_fn,\n",
    "        (env_state, policy_params, random_key),\n",
    "        (),\n",
    "        length=episode_length\n",
    "    )\n",
    "    return env_state, transitions\n",
    "\n",
    "\n",
    "def scoring_function(\n",
    "        policies_params,\n",
    "        random_key,\n",
    "        play_step_fn,\n",
    "        play_reset_fn,\n",
    "        episode_length,\n",
    "):\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "    keys = jax.random.split(\n",
    "        subkey, jax.tree_util.tree_leaves(policies_params)[0].shape[0]\n",
    "    )\n",
    "    reset_fn = jax.vmap(play_reset_fn)\n",
    "    init_states = reset_fn(keys)\n",
    "\n",
    "    unroll_fn = functools.partial(\n",
    "        generate_unroll,\n",
    "        episode_length=episode_length,\n",
    "        play_step_fn=play_step_fn,\n",
    "        random_key=subkey,\n",
    "    )\n",
    "\n",
    "    # jax.debug.print(\"init_states {a}\", a=init_states.obs.shape)\n",
    "\n",
    "    # print(\"num_pol:\",jax.tree_util.tree_leaves(policies_params)[0].shape[0] )\n",
    "    # print(\"init state\", init_states.obs.shape)\n",
    "\n",
    "    env_states, data = jax.vmap(unroll_fn)(\n",
    "        policies_params,\n",
    "        init_states,\n",
    "    )\n",
    "\n",
    "    # create a mask to extract data properly\n",
    "    is_done = jnp.clip(jnp.cumsum(data.dones, axis=1), 0, 1)\n",
    "    mask = jnp.roll(is_done, 1, axis=1)\n",
    "    mask = mask.at[:, 0].set(0)\n",
    "\n",
    "    # scores\n",
    "    fitnesses = jnp.sum(data.rewards * (1.0 - mask), axis=1)\n",
    "\n",
    "    return fitnesses, data, random_key\n",
    "\n",
    "\n",
    "scoring_fn = functools.partial(\n",
    "    scoring_function,\n",
    "    play_step_fn=play_step_fn,\n",
    "    play_reset_fn=env.reset,\n",
    "    episode_length=episode_length,\n",
    ")\n",
    "\n",
    "\n",
    "print(env.get_action_sizes())\n",
    "print(env.get_obs_sizes())\n",
    "\n",
    "random_key = jax.random.PRNGKey(seed)\n",
    "num_agents = len(env.get_action_sizes())\n",
    "\n",
    "# Make sure to pass the correct config parameters\n",
    "matd3_config = CEMMATD3Config(\n",
    "    num_agents=len(env.get_action_sizes()),\n",
    "    episode_length=episode_length,\n",
    "    batch_size=batch_size,\n",
    "    policy_delay=policy_delay,\n",
    "    soft_tau_update=soft_tau_update,\n",
    "    expl_noise=expl_noise,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_sizes,  \n",
    "    policy_hidden_layer_size=policy_hidden_layer_sizes,  \n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    policy_learning_rate=policy_learning_rate,\n",
    "    discount=discount,\n",
    "    noise_clip=noise_clip,\n",
    "    policy_noise=policy_noise,\n",
    "    reward_scaling=reward_scaling,\n",
    "    max_grad_norm=max_grad_norm\n",
    ")\n",
    "\n",
    "cem_matd3_agent = CEMMATD3(config=matd3_config, env=env, scoring_function=scoring_fn)\n",
    "\n",
    "training_state = cem_matd3_agent.init(random_key=random_key)\n",
    "\n",
    "reset_fn = jax.vmap(env.reset)\n",
    "\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, env_batch_size)\n",
    "\n",
    "env_states = reset_fn(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614e27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init replay buffer\n",
    "dummmy_transition = Transition.init_dummy(observation_dim=env.observation_size, action_dim=env.action_size)\n",
    "\n",
    "replay_buffer = ReplayBuffer.init(buffer_size=replay_buffer_size,  transition=dummmy_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b63997",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer, training_state = warmstart_buffer(\n",
    "    env=env,\n",
    "    replay_buffer=replay_buffer,\n",
    "    training_state=training_state,\n",
    "    warmstart_play_step_fn = warmstart_play_step_fn,\n",
    "    warmstart_steps=warmstart_steps,\n",
    "    env_batch_size=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847cdc9c",
   "metadata": {},
   "source": [
    "## Training/logging loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759602d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tin/anaconda3/envs/mix-me/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtherealtin\u001b[0m (\u001b[33mtherealtin-uit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/tin/anaconda3/envs/mix-me/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:82: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd16cd7a2e55410ead9acead29d9e61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668288416667565, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tin/Desktop/HaiDang/RL/Mix-ME/MA-QDax/examples/scripts/wandb/run-20251113_210257-gp5j0gpc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/therealtin-uit/matd3-multiagent-rl/runs/gp5j0gpc' target=\"_blank\">CEMMATD3_hopper_uni_20251113_210255</a></strong> to <a href='https://wandb.ai/therealtin-uit/matd3-multiagent-rl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/therealtin-uit/matd3-multiagent-rl' target=\"_blank\">https://wandb.ai/therealtin-uit/matd3-multiagent-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/therealtin-uit/matd3-multiagent-rl/runs/gp5j0gpc' target=\"_blank\">https://wandb.ai/therealtin-uit/matd3-multiagent-rl/runs/gp5j0gpc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Total timesteps: 7,864,320\n",
      "  Env batch size: 128\n",
      "  Total iterations: 1,000\n",
      "  Log period: 10\n",
      "  Number of training loops: 100\n",
      "  Warmstart steps: 25,600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tin/Desktop/HaiDang/RL/Mix-ME/MA-QDax/qdax/baselines/cem_matd3.py:680: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  selected_offsprings = jax.tree_map(lambda x: x[:self._num_learning_offspring], offsprings)\n",
      "/home/tin/Desktop/HaiDang/RL/Mix-ME/MA-QDax/qdax/baselines/cem_matd3.py:705: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  new_offsprings = jax.tree_map(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop    0/100 | Steps:  125,600 | Fitness:  206.74 | Time:  27.36s | TPS:     46.8\n",
      "Loop    1/100 | Steps:  225,600 | Fitness:  221.67 | Time:  24.84s | TPS:     51.5\n",
      "Loop    2/100 | Steps:  325,600 | Fitness:  238.00 | Time:  26.50s | TPS:     48.3\n",
      "Loop    3/100 | Steps:  425,600 | Fitness:  335.20 | Time:  26.16s | TPS:     48.9\n",
      "Loop    4/100 | Steps:  525,600 | Fitness:  523.78 | Time:  27.09s | TPS:     47.2\n",
      "Loop    5/100 | Steps:  625,600 | Fitness:  474.54 | Time:  25.77s | TPS:     49.7\n",
      "Loop    6/100 | Steps:  725,600 | Fitness:  536.76 | Time:  26.28s | TPS:     48.7\n",
      "Loop    7/100 | Steps:  825,600 | Fitness:  351.44 | Time:  27.64s | TPS:     46.3\n",
      "Loop    8/100 | Steps:  925,600 | Fitness:  355.10 | Time:  27.71s | TPS:     46.2\n",
      "Loop    9/100 | Steps: 1,025,600 | Fitness:  381.70 | Time:  27.29s | TPS:     46.9\n",
      "Loop   10/100 | Steps: 1,125,600 | Fitness:  397.64 | Time:  27.42s | TPS:     46.7\n",
      "Loop   11/100 | Steps: 1,225,600 | Fitness:  409.73 | Time:  26.66s | TPS:     48.0\n",
      "Loop   12/100 | Steps: 1,325,600 | Fitness:  403.41 | Time:  26.96s | TPS:     47.5\n",
      "Loop   13/100 | Steps: 1,425,600 | Fitness:  437.75 | Time:  26.49s | TPS:     48.3\n",
      "Loop   14/100 | Steps: 1,525,600 | Fitness:  433.94 | Time:  25.92s | TPS:     49.4\n",
      "Loop   15/100 | Steps: 1,625,600 | Fitness:  434.80 | Time:  26.03s | TPS:     49.2\n",
      "Loop   16/100 | Steps: 1,725,600 | Fitness:  426.82 | Time:  26.21s | TPS:     48.8\n",
      "Loop   17/100 | Steps: 1,825,600 | Fitness:  494.84 | Time:  26.07s | TPS:     49.1\n",
      "Loop   18/100 | Steps: 1,925,600 | Fitness:  479.39 | Time:  26.18s | TPS:     48.9\n",
      "Loop   19/100 | Steps: 2,025,600 | Fitness:  506.87 | Time:  26.53s | TPS:     48.2\n",
      "Loop   20/100 | Steps: 2,125,600 | Fitness:  485.68 | Time:  25.93s | TPS:     49.4\n",
      "Loop   21/100 | Steps: 2,225,600 | Fitness:  498.45 | Time:  25.94s | TPS:     49.4\n",
      "Loop   22/100 | Steps: 2,325,600 | Fitness:  507.75 | Time:  26.02s | TPS:     49.2\n",
      "Loop   23/100 | Steps: 2,425,600 | Fitness:  459.75 | Time:  26.20s | TPS:     48.9\n",
      "Loop   24/100 | Steps: 2,525,600 | Fitness:  448.72 | Time:  25.83s | TPS:     49.6\n",
      "Loop   25/100 | Steps: 2,625,600 | Fitness:  450.11 | Time:  25.91s | TPS:     49.4\n",
      "Loop   26/100 | Steps: 2,725,600 | Fitness:  450.35 | Time:  25.84s | TPS:     49.5\n",
      "Loop   27/100 | Steps: 2,825,600 | Fitness:  465.58 | Time:  27.82s | TPS:     46.0\n",
      "Loop   28/100 | Steps: 2,925,600 | Fitness:  511.39 | Time:  25.26s | TPS:     50.7\n",
      "Loop   29/100 | Steps: 3,025,600 | Fitness:  536.06 | Time:  25.26s | TPS:     50.7\n",
      "Loop   30/100 | Steps: 3,125,600 | Fitness:  691.60 | Time:  25.45s | TPS:     50.3\n",
      "Loop   31/100 | Steps: 3,225,600 | Fitness:  707.84 | Time:  25.36s | TPS:     50.5\n",
      "Loop   32/100 | Steps: 3,325,600 | Fitness:  757.05 | Time:  25.58s | TPS:     50.0\n",
      "Loop   33/100 | Steps: 3,425,600 | Fitness:  976.41 | Time:  25.40s | TPS:     50.4\n",
      "Loop   34/100 | Steps: 3,525,600 | Fitness:  407.37 | Time:  25.41s | TPS:     50.4\n",
      "Loop   35/100 | Steps: 3,625,600 | Fitness:  452.18 | Time:  25.33s | TPS:     50.5\n",
      "Loop   36/100 | Steps: 3,725,600 | Fitness:  545.91 | Time:  25.65s | TPS:     49.9\n",
      "Loop   37/100 | Steps: 3,825,600 | Fitness:  578.98 | Time:  25.41s | TPS:     50.4\n",
      "Loop   38/100 | Steps: 3,925,600 | Fitness:  589.08 | Time:  25.32s | TPS:     50.5\n",
      "Loop   39/100 | Steps: 4,025,600 | Fitness:  509.12 | Time:  25.52s | TPS:     50.2\n",
      "Loop   40/100 | Steps: 4,125,600 | Fitness:  521.21 | Time:  25.52s | TPS:     50.2\n",
      "Loop   41/100 | Steps: 4,225,600 | Fitness:  626.82 | Time:  25.29s | TPS:     50.6\n",
      "Loop   42/100 | Steps: 4,325,600 | Fitness:  591.24 | Time:  25.24s | TPS:     50.7\n",
      "Loop   43/100 | Steps: 4,425,600 | Fitness:  567.14 | Time:  25.49s | TPS:     50.2\n",
      "Loop   44/100 | Steps: 4,525,600 | Fitness:  563.91 | Time:  30.27s | TPS:     42.3\n",
      "Loop   45/100 | Steps: 4,625,600 | Fitness:  464.51 | Time:  28.24s | TPS:     45.3\n",
      "Loop   46/100 | Steps: 4,725,600 | Fitness:  556.46 | Time:  28.74s | TPS:     44.5\n",
      "Loop   47/100 | Steps: 4,825,600 | Fitness:  550.61 | Time:  26.14s | TPS:     49.0\n",
      "Loop   48/100 | Steps: 4,925,600 | Fitness:  584.40 | Time:  25.93s | TPS:     49.4\n",
      "Loop   49/100 | Steps: 5,025,600 | Fitness:  597.02 | Time:  26.84s | TPS:     47.7\n",
      "Loop   50/100 | Steps: 5,125,600 | Fitness:  636.16 | Time:  25.97s | TPS:     49.3\n",
      "Loop   51/100 | Steps: 5,225,600 | Fitness:  619.71 | Time:  26.34s | TPS:     48.6\n",
      "Loop   52/100 | Steps: 5,325,600 | Fitness:  513.31 | Time:  27.79s | TPS:     46.1\n",
      "Loop   53/100 | Steps: 5,425,600 | Fitness:  539.15 | Time:  27.72s | TPS:     46.2\n",
      "Loop   54/100 | Steps: 5,525,600 | Fitness:  605.49 | Time:  25.68s | TPS:     49.9\n",
      "Loop   55/100 | Steps: 5,625,600 | Fitness:  577.22 | Time:  25.61s | TPS:     50.0\n",
      "Loop   56/100 | Steps: 5,725,600 | Fitness:  571.96 | Time:  25.52s | TPS:     50.2\n",
      "Loop   57/100 | Steps: 5,825,600 | Fitness:  661.59 | Time:  27.74s | TPS:     46.1\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize wandb with proper run naming\n",
    "def init_wandb_logging():\n",
    "    \"\"\"Initialize wandb with descriptive run name\"\"\"\n",
    "    run_name = f\"CEMMATD3_{env_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"matd3-multiagent-rl\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            # Environment config\n",
    "            \"env_name\": env_name,\n",
    "            \"episode_length\": episode_length,\n",
    "            \"num_agents\": num_agents,\n",
    "            \"parameter_sharing\": parameter_sharing,\n",
    "            \"emitter_type\": emitter_type,\n",
    "            \"homogenisation_method\": homogenisation_method,\n",
    "            \n",
    "            # Training config\n",
    "            \"num_timesteps\": num_timesteps,\n",
    "            \"env_batch_size\": env_batch_size,\n",
    "            \"warmstart_steps\": warmstart_steps,\n",
    "            \"grad_updates_per_step\": grad_updates_per_step,\n",
    "            \"log_period\": log_interval,\n",
    "            \"num_evals\": num_evals,\n",
    "            \n",
    "            # MATD3 hyperparameters\n",
    "            \"batch_size\": batch_size,\n",
    "            \"policy_learning_rate\": policy_learning_rate,\n",
    "            \"critic_learning_rate\": critic_learning_rate,\n",
    "            \"discount\": discount,\n",
    "            \"soft_tau_update\": soft_tau_update,\n",
    "            \"policy_delay\": policy_delay,\n",
    "            \"expl_noise\": expl_noise,\n",
    "            \"noise_clip\": noise_clip,\n",
    "            \"reward_scaling\": reward_scaling,\n",
    "            \"replay_buffer_size\": replay_buffer_size,\n",
    "            \n",
    "            # Network architecture\n",
    "            \"policy_hidden_layer_sizes\": policy_hidden_layer_sizes,\n",
    "            \"critic_hidden_layer_sizes\": critic_hidden_layer_sizes,\n",
    "            \n",
    "            # Other\n",
    "            \"seed\": seed,\n",
    "        },\n",
    "        tags=[\"cemmatd3\", \"multiagent\", env_name.split(\"_\")[0]]\n",
    "    )\n",
    "\n",
    "# Alternative: Pass parameters explicitly to the function\n",
    "def run_training_loop_with_logging_v2(training_state, replay_buffer, env_states):\n",
    "    \"\"\"Complete training loop with wandb logging and error handling - version with explicit parameters\"\"\"\n",
    "    \n",
    "    # Initialize wandb\n",
    "    init_wandb_logging()\n",
    "    \n",
    "    try:\n",
    "        # Calculate training parameters\n",
    "        num_loops = int(num_iterations / log_interval)\n",
    "        \n",
    "        print(f\"Training Configuration:\")\n",
    "        print(f\"  Total timesteps: {num_timesteps:,}\")\n",
    "        print(f\"  Env batch size: {env_batch_size}\")\n",
    "        print(f\"  Total iterations: {num_iterations:,}\")\n",
    "        print(f\"  Log period: {log_interval}\")\n",
    "        print(f\"  Number of training loops: {num_loops}\")\n",
    "        print(f\"  Warmstart steps: {warmstart_steps:,}\")\n",
    "        \n",
    "        # Initialize random key for evaluation\n",
    "        random_key_local = jax.random.PRNGKey(seed + 1000)  # Different seed for eval\n",
    "        \n",
    "        # Training metrics tracking\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(num_loops):\n",
    "            loop_start_time = time.time()\n",
    "            \n",
    "            # Training step\n",
    "            (training_state, replay_buffer), train_metrics = jax.lax.scan(\n",
    "                cem_matd3_agent.scan_update,\n",
    "                (training_state, replay_buffer),\n",
    "                (),\n",
    "                length=log_interval,\n",
    "            )\n",
    "            train_metrics = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.mean(x), train_metrics\n",
    "            )\n",
    "            # Evaluation\n",
    "            training_state, eval_metrics = cem_matd3_agent.evaluate(training_state)\n",
    "\n",
    "            metrics = train_metrics | eval_metrics\n",
    "        \n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            current_timesteps = warmstart_steps + (i + 1) * population_size * log_interval * episode_length\n",
    "            loop_time = time.time() - loop_start_time\n",
    "            total_time = time.time() - start_time\n",
    "            timesteps_per_second = (env_batch_size * log_interval) / loop_time\n",
    "            \n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                \"training/timesteps\": current_timesteps,\n",
    "                \"training/loop\": i,\n",
    "                \"evaluation/mean_return\": metrics[\"center_fitness_average\"],\n",
    "                \"evaluation/return_std\": metrics[\"center_fitness_std\"],\n",
    "\n",
    "                \"performance/timesteps_per_second\": timesteps_per_second,\n",
    "                \"performance/loop_time\": loop_time,\n",
    "                \"performance/total_time\": total_time,\n",
    "                \"training/replay_buffer_size\": replay_buffer.current_size,\n",
    "                \"training/training_steps\": training_state.steps,\n",
    "                \"rl_in_elites_percentage\": metrics[\"rl_in_elites_percentage\"]\n",
    "            })\n",
    "            \n",
    "            # Console output\n",
    "            print(f\"Loop {i:4d}/{num_loops} | \"\n",
    "                  f\"Steps: {current_timesteps:8,} | \"\n",
    "                  f\"Fitness: {metrics['center_fitness_average']:7.2f} | \"\n",
    "                  f\"Time: {loop_time:6.2f}s | \"\n",
    "                  f\"TPS: {timesteps_per_second:8.1f}\")\n",
    "        \n",
    "        \n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Total time: {total_time:.2f}s\")\n",
    "        print(f\"Final training steps: {training_state.steps}\")\n",
    "        \n",
    "        # Final logging\n",
    "        wandb.log({\n",
    "            \"final/total_time\": total_time,\n",
    "        })\n",
    "        \n",
    "        return training_state, replay_buffer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed with error: {e}\")\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        wandb.finish()\n",
    "\n",
    "# Run the training loop - use the version that passes parameters explicitly\n",
    "final_training_state, final_replay_buffer = run_training_loop_with_logging_v2(\n",
    "    training_state, replay_buffer, env_states\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mix-me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
