{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d03126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 19:35:13.445536: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.9.86). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from brax.envs import State as EnvState\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Tuple, List\n",
    "\n",
    "from qdax import environments\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs\n",
    "from qdax.core.neuroevolution.buffers.buffer import (\n",
    "    QDTransition,\n",
    "    ReplayBuffer,\n",
    "    Transition,\n",
    ")\n",
    "from qdax.core.neuroevolution.sac_td3_utils import warmstart_buffer, generate_unroll, do_iteration_fn\n",
    "from qdax.core.neuroevolution.buffers.buffer import ReplayBuffer, Transition\n",
    "from qdax.core.neuroevolution.mdp_utils import TrainingState\n",
    "from qdax.custom_types import Metrics\n",
    "from qdax.custom_types import (\n",
    "    Action,\n",
    "    Descriptor,\n",
    "    Mask,\n",
    "    Metrics,\n",
    "    Observation,\n",
    "    Params,\n",
    "    Reward,\n",
    "    RNGKey,\n",
    ")\n",
    "\n",
    "# Multiagent imports\n",
    "from qdax.environments.multi_agent_wrappers import MultiAgentBraxWrapper\n",
    "# jax.config.update(\"jax_log_compiles\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245481c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "env_name = 'walker2d_uni'#@param['ant_uni', 'hopper_uni', 'walker_uni', 'halfcheetah_uni', 'humanoid_uni', 'ant_omni', 'humanoid_omni']\n",
    "parameter_sharing=False\n",
    "emitter_type=\"mix\"\n",
    "homogenisation_method=\"concat\"\n",
    "episode_length = 1000 #@param {type:\"integer\"}\n",
    "num_timesteps = 7_864_320 #@param {type:\"integer\"}\n",
    "# num_timesteps = 1_000_000 #@param {type:\"integer\"}\n",
    "seed = 0 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (64, 64) #@param {type:\"raw\"}\n",
    "policy_learning_rate = 3e-4\n",
    "critic_learning_rate = 3e-4\n",
    "alpha_learning_rate = 3e-4\n",
    "num_init_cvt_samples = 50000 #@param {type:\"integer\"}\n",
    "num_centroids = 1024 #@param {type:\"integer\"}\n",
    "min_bd = 0. #@param {type:\"number\"}\n",
    "max_bd = 1.0 #@param {type:\"number\"}\n",
    "warmstart_steps=8192*10\n",
    "num_evals=20    \n",
    "log_period=1024\n",
    "\n",
    "# SAC params\n",
    "env_batch_size = 128 #@param {type:\"number\"}\n",
    "batch_size=128\n",
    "grad_updates_per_step=0.3 #@param {type:\"number\"}\n",
    "replay_buffer_size = 1000000 #@param {type:\"number\"}\n",
    "critic_hidden_layer_sizes = (256, 256) #@param {type:\"raw\"}\n",
    "discount = 0.99 #@param {type:\"number\"}\n",
    "reward_scaling = 1.0 #@param {type:\"number\"}s\n",
    "tau = 0.005 #@param {type:\"number\"}  # SAC uses tau instead of soft_tau_update\n",
    "alpha_init=1.0 #@param {type:\"number\"}\n",
    "fix_alpha=False #@param {type:\"boolean\"}\n",
    "normalize_observations=False #@param {type:\"boolean\"}\n",
    "max_grad_norm=0.0 #@param {type:\"number\"}\n",
    "policy_delay = 4\n",
    "target_entropy_scale = 0.8\n",
    "independent_std = False\n",
    "use_layer_norm = True\n",
    "use_per_agent_alpha: bool = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41c2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdax.core.neuroevolution.networks.masac_networks import make_masac_networks\n",
    "from qdax.core.neuroevolution.losses.masac_loss import masac_critic_loss_fn, masac_policy_loss_fn, masac_alpha_loss_fn\n",
    "from qdax.baselines.masac import MASAC, MASacConfig, MASacTrainingState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8d5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Warmstart related functions\n",
    "import functools\n",
    "\n",
    "# Define the random step function\n",
    "def warmstart_play_step_fn(\n",
    "    env_state: EnvState,\n",
    "    random_key: RNGKey,\n",
    "    env: MultiAgentBraxWrapper,\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "\n",
    "    action_sizes = env.get_action_sizes()\n",
    "\n",
    "    keys = jax.random.split(subkey, len(action_sizes))\n",
    "\n",
    "    actions = {\n",
    "        agent_idx: jax.random.uniform(agent_key, (size,), minval=-1, maxval=1)\n",
    "        for (agent_idx, size), agent_key in zip(action_sizes.items(), keys)\n",
    "    }\n",
    "\n",
    "    flatten_actions = jnp.concatenate([a for a in actions.values()])\n",
    "\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=next_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        actions=flatten_actions,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "    )\n",
    "\n",
    "    return next_state, random_key, transition\n",
    "\n",
    "def generate_unroll_warmstart(\n",
    "    random_key: RNGKey,\n",
    "    env_state: EnvState,\n",
    "    env: MultiAgentBraxWrapper,\n",
    "    warmstart_play_step_fn: Callable[\n",
    "        [EnvState, RNGKey, MultiAgentBraxWrapper],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    "    warmstart_steps: int,\n",
    ") -> Tuple[EnvState, Transition]:\n",
    "    \"\"\"Pre-populates the buffer with transitions. Returns the warmstarted buffer\n",
    "    and the new state of the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def _scan_play_step_fn(\n",
    "        carry: Tuple[EnvState, RNGKey], unused_arg: Any\n",
    "    ) -> Tuple[Tuple[EnvState, RNGKey], Transition]:\n",
    "        env_state, random_key, transitions = warmstart_play_step_fn(*carry, env)\n",
    "        return (env_state, random_key), transitions\n",
    "\n",
    "    (env_state, random_key), transitions = jax.lax.scan(\n",
    "        _scan_play_step_fn,\n",
    "        (env_state, random_key),\n",
    "        (),\n",
    "        length=warmstart_steps\n",
    "    )\n",
    "\n",
    "    return env_state, transitions\n",
    "\n",
    "@functools.partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"env\", \"warmstart_play_step_fn\", \"warmstart_steps\", \"env_batch_size\")\n",
    ")\n",
    "def warmstart_buffer(\n",
    "    env: MultiAgentBraxWrapper,\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    training_state: MASacTrainingState,\n",
    "    warmstart_play_step_fn: Callable[\n",
    "        [EnvState, RNGKey, MultiAgentBraxWrapper],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    "    warmstart_steps: int,\n",
    "    env_batch_size: int,\n",
    "):\n",
    "    \n",
    "    generate_unroll = functools.partial(\n",
    "        generate_unroll_warmstart,\n",
    "        env = env,\n",
    "        warmstart_play_step_fn=warmstart_play_step_fn,\n",
    "        warmstart_steps=warmstart_steps//env_batch_size\n",
    "    )\n",
    "\n",
    "    generate_unroll_vmap = jax.vmap(\n",
    "        generate_unroll,\n",
    "        in_axes=(0, 0)\n",
    "    )\n",
    "\n",
    "    random_key, subkey = jax.random.split(training_state.random_key)\n",
    "    keys = jax.random.split(subkey, env_batch_size)\n",
    "\n",
    "    training_state = training_state.replace(\n",
    "        random_key=random_key\n",
    "    )\n",
    "\n",
    "    reset_fn = jax.vmap(env.reset)\n",
    "\n",
    "    env_states = reset_fn(keys)\n",
    "\n",
    "    random_key, subkey = jax.random.split(training_state.random_key)\n",
    "\n",
    "    training_state = training_state.replace(\n",
    "        random_key=random_key\n",
    "    )\n",
    "\n",
    "    keys = jax.random.split(subkey, env_batch_size)\n",
    "    env_states, transitions = generate_unroll_vmap(keys, env_states)\n",
    "\n",
    "    replay_buffer = replay_buffer.insert(transitions)\n",
    "    \n",
    "    return replay_buffer, training_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097fca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env_name = env_name.split(\"_\")[0]\n",
    "env = environments.create(env_name, episode_length=episode_length)\n",
    "env = MultiAgentBraxWrapper(\n",
    "    env,\n",
    "    env_name=base_env_name,\n",
    "    parameter_sharing=False,\n",
    "    emitter_type=emitter_type,\n",
    "    homogenisation_method=homogenisation_method\n",
    ")\n",
    "\n",
    "random_key = jax.random.PRNGKey(seed)\n",
    "num_agents = len(env.get_action_sizes())\n",
    "\n",
    "# Make sure to pass the correct config parameters\n",
    "masac_config = MASacConfig(\n",
    "    num_agents=len(env.get_action_sizes()),\n",
    "    episode_length=episode_length,\n",
    "    batch_size=batch_size,\n",
    "    tau=tau,  # SAC uses tau instead of soft_tau_update\n",
    "    normalize_observations=normalize_observations,\n",
    "    policy_learning_rate=policy_learning_rate,  # SAC typically uses same LR for all components\n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    alpha_learning_rate=alpha_learning_rate,\n",
    "    alpha_init=alpha_init,\n",
    "    discount=discount,\n",
    "    reward_scaling=reward_scaling,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_sizes,\n",
    "    policy_hidden_layer_size=policy_hidden_layer_sizes,\n",
    "    fix_alpha=fix_alpha,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    target_entropy_scale=target_entropy_scale,\n",
    "    independent_std=independent_std,\n",
    "    policy_delay=policy_delay,\n",
    "    use_layer_norm=use_layer_norm,\n",
    "    use_per_agent_alpha=use_per_agent_alpha\n",
    ")\n",
    "\n",
    "masac_agent = MASAC(config=masac_config, action_sizes=env.get_action_sizes())\n",
    "\n",
    "training_state = masac_agent.init(\n",
    "    random_key=random_key, \n",
    "    action_sizes_each_agent=env.get_action_sizes(),\n",
    "    observation_size_raw=env.observation_size,\n",
    "    observation_sizes_each_agent=env.get_obs_sizes()\n",
    ")\n",
    "\n",
    "reset_fn = jax.vmap(env.reset)\n",
    "\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, env_batch_size)\n",
    "\n",
    "env_states = reset_fn(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f6f216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7596648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(env.action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d599a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init replay buffer\n",
    "dummy_transition = QDTransition.init_dummy(\n",
    "    observation_dim=env.observation_size, \n",
    "    action_dim=env.action_size, \n",
    "    descriptor_dim=env.behavior_descriptor_length\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.init(buffer_size=replay_buffer_size, transition=dummy_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56aade8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer, training_state = warmstart_buffer(\n",
    "    env=env,\n",
    "    replay_buffer=replay_buffer,\n",
    "    training_state=training_state,\n",
    "    warmstart_play_step_fn=warmstart_play_step_fn,\n",
    "    warmstart_steps=warmstart_steps,\n",
    "    env_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2343dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_step_fn = functools.partial(\n",
    "    masac_agent.play_qd_step_fn,\n",
    "    env=env,\n",
    "    deterministic=False\n",
    ")\n",
    "\n",
    "# Create the scan_update function\n",
    "@functools.partial(jax.jit, static_argnames=(\"unflatten_obs_fn\", \"unflatten_actions_fn\"))\n",
    "def scan_update(\n",
    "    carry: Tuple[MASacTrainingState, ReplayBuffer],\n",
    "    unused: Any,\n",
    "    unflatten_obs_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    "    unflatten_actions_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]]\n",
    ") -> Tuple[Tuple[MASacTrainingState, ReplayBuffer], Metrics]:\n",
    "    \"\"\"Single update step for the scan operation\"\"\"\n",
    "    training_state, replay_buffer = carry\n",
    "    \n",
    "    # Perform one update step\n",
    "    new_training_state, new_replay_buffer, metrics = masac_agent.update(\n",
    "        training_state, \n",
    "        replay_buffer,\n",
    "        unflatten_obs_fn,\n",
    "        unflatten_actions_fn\n",
    "    )\n",
    "    \n",
    "    return (new_training_state, new_replay_buffer), metrics\n",
    "\n",
    "# Now create the clean single_step_and_update function\n",
    "@functools.partial(jax.jit, static_argnames=(\"play_step_fn\", \"unflatten_obs_fn\", \"unflatten_actions_fn\", \"num_updates\"))\n",
    "def single_step_and_update(\n",
    "    carry: [EnvState, MASacTrainingState, ReplayBuffer],\n",
    "    _,\n",
    "    play_step_fn: Callable[\n",
    "        [EnvState, MASacTrainingState],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            MASacTrainingState,\n",
    "            QDTransition,\n",
    "        ],\n",
    "    ],\n",
    "    unflatten_obs_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    "    unflatten_actions_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    "    num_updates: int\n",
    ") -> Tuple[Tuple[EnvState, MASacTrainingState, ReplayBuffer], Metrics]:\n",
    "    \"\"\"Performs one environment step followed by multiple gradient updates\"\"\"\n",
    "    \n",
    "    # Vectorized environment step\n",
    "    play_step_fn_vmap = jax.vmap(\n",
    "        play_step_fn, \n",
    "        in_axes=(0, None), \n",
    "        out_axes=(0, None, 0)\n",
    "    )\n",
    "\n",
    "    env_states, training_state, replay_buffer = carry\n",
    "\n",
    "    env_states, training_state, transitions = play_step_fn_vmap(env_states, training_state)\n",
    "\n",
    "    # Insert transitions into replay buffer\n",
    "    replay_buffer = replay_buffer.insert(transitions)\n",
    "    \n",
    "    # Create partial function for scan_update with fixed unflatten functions\n",
    "    scan_update_partial = functools.partial(\n",
    "        scan_update, \n",
    "        unflatten_obs_fn=unflatten_obs_fn, \n",
    "        unflatten_actions_fn=unflatten_actions_fn,\n",
    "    )\n",
    "\n",
    "    # Perform multiple gradient updates\n",
    "    (training_state, replay_buffer), metrics = jax.lax.scan(\n",
    "        scan_update_partial,\n",
    "        (training_state, replay_buffer),\n",
    "        (),\n",
    "        length=num_updates\n",
    "    )\n",
    "\n",
    "    return (env_states, training_state, replay_buffer), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a72c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten_obs_fn(global_obs: jnp.ndarray, env: MultiAgentBraxWrapper) -> dict[int, jnp.ndarray]:\n",
    "    agent_obs = {}\n",
    "    for agent_idx, obs_indices in env.agent_obs_mapping.items():\n",
    "        agent_obs[agent_idx] = global_obs[obs_indices]\n",
    "    return agent_obs\n",
    "\n",
    "def unflatten_actions_fn(flatten_action: jnp.ndarray, env: MultiAgentBraxWrapper) -> dict[int, jax.Array]:\n",
    "    \"\"\"Because the actions in the form of Dict[int, jnp.array] is flatten by \n",
    "    flatten_actions = jnp.concatenate([a for a in actions.values()]) so we do this way\n",
    "    \"\"\"\n",
    "    actions = {}\n",
    "    start = 0\n",
    "    for agent_idx, size in env.get_action_sizes().items():\n",
    "        end = start + size\n",
    "        actions[agent_idx] = flatten_action[start:end]\n",
    "        start = end\n",
    "    return actions\n",
    "\n",
    "unflatten_obs_fn = functools.partial(\n",
    "    unflatten_obs_fn,\n",
    "    env=env\n",
    ")\n",
    "\n",
    "unflatten_actions_fn = functools.partial(\n",
    "    unflatten_actions_fn,\n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd9dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_and_update = functools.partial(\n",
    "    single_step_and_update,\n",
    "    play_step_fn=play_step_fn,\n",
    "    unflatten_obs_fn=unflatten_obs_fn,\n",
    "    unflatten_actions_fn=unflatten_actions_fn,\n",
    "    num_updates=int(grad_updates_per_step * env_batch_size)\n",
    ")\n",
    "\n",
    "\n",
    "play_eval_step_fn = functools.partial(\n",
    "    masac_agent.play_qd_step_fn,\n",
    "    env=env,\n",
    "    deterministic=True\n",
    "    # deterministic=False\n",
    ")\n",
    "\n",
    "play_eval_step_fn = jax.vmap(\n",
    "    play_eval_step_fn,\n",
    "    in_axes=(0, None),\n",
    "    out_axes=(0, None, 0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbc3b6",
   "metadata": {},
   "source": [
    "## Single critic and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f3457d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_state.policy_params[0][\"params\"][\"log_std\"])\n",
    "# print(jnp.exp(training_state.policy_params[0][\"params\"][\"log_std\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22faef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for agent_idx, p in enumerate(training_state.policy_params):\n",
    "#     print(f\"agent/agent_{agent_idx}_std {jnp.mean(jnp.array(jnp.exp(p['params']['log_std'])))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e1b13ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tin/anaconda3/envs/mix-me/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtherealtin\u001b[0m (\u001b[33mtherealtin-uit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/tin/anaconda3/envs/mix-me/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:70: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tin/Desktop/HaiDang/RL/Mix-ME/MA-QDax/examples/scripts/wandb/run-20251027_193527-a5xjcdvw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/therealtin-uit/masac-multiagent-rl/runs/a5xjcdvw' target=\"_blank\">MASAC_walker2d_uni_20251027_193525</a></strong> to <a href='https://wandb.ai/therealtin-uit/masac-multiagent-rl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/therealtin-uit/masac-multiagent-rl' target=\"_blank\">https://wandb.ai/therealtin-uit/masac-multiagent-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/therealtin-uit/masac-multiagent-rl/runs/a5xjcdvw' target=\"_blank\">https://wandb.ai/therealtin-uit/masac-multiagent-rl/runs/a5xjcdvw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Total timesteps: 7,864,320\n",
      "  Env batch size: 128\n",
      "  Total iterations: 61,440\n",
      "  Log period: 1024\n",
      "  Number of training loops: 60\n",
      "  Warmstart steps: 81,920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tin/Desktop/HaiDang/RL/Mix-ME/MA-QDax/qdax/core/neuroevolution/mdp_utils.py:83: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(mask_episodes, transition)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop    0/60 | Steps:  212,992 | Fitness:  276.07 | Critic Loss:  15.3932 | Actor Losses: -31.7225 | Alpha Loss: 0.3589 | Alpha Mean: 0.3826 | Time:  32.25s | TPS:   4064.5\n",
      "Loop    1/60 | Steps:  344,064 | Fitness:  292.02 | Critic Loss:  13.1206 | Actor Losses: -34.9238 | Alpha Loss: 0.0185 | Alpha Mean: 0.0555 | Time:  23.84s | TPS:   5497.2\n",
      "Loop    2/60 | Steps:  475,136 | Fitness:  355.54 | Critic Loss:   4.6526 | Actor Losses: -30.1149 | Alpha Loss: 0.0002 | Alpha Mean: 0.0175 | Time:  17.70s | TPS:   7404.7\n",
      "Loop    3/60 | Steps:  606,208 | Fitness:  341.70 | Critic Loss:   3.7378 | Actor Losses: -30.0489 | Alpha Loss: 0.0002 | Alpha Mean: 0.0131 | Time:  17.73s | TPS:   7394.7\n",
      "Loop    4/60 | Steps:  737,280 | Fitness:  338.62 | Critic Loss:   3.0460 | Actor Losses: -30.9092 | Alpha Loss: 0.0000 | Alpha Mean: 0.0115 | Time:  17.65s | TPS:   7427.0\n",
      "Loop    5/60 | Steps:  868,352 | Fitness:  359.38 | Critic Loss:   2.2328 | Actor Losses: -31.2606 | Alpha Loss: 0.0000 | Alpha Mean: 0.0101 | Time:  17.70s | TPS:   7407.0\n",
      "Loop    6/60 | Steps:  999,424 | Fitness:  341.96 | Critic Loss:   1.9008 | Actor Losses: -31.2460 | Alpha Loss: 0.0001 | Alpha Mean: 0.0085 | Time:  17.73s | TPS:   7393.0\n",
      "Checkpoint at loop 6 - Best fitness: 359.38\n",
      "Loop    7/60 | Steps: 1,130,496 | Fitness:  340.47 | Critic Loss:   1.3923 | Actor Losses: -32.0057 | Alpha Loss: 0.0000 | Alpha Mean: 0.0079 | Time:  17.78s | TPS:   7372.5\n",
      "Loop    8/60 | Steps: 1,261,568 | Fitness:  343.95 | Critic Loss:   0.4987 | Actor Losses: -32.3783 | Alpha Loss: 0.0001 | Alpha Mean: 0.0072 | Time:  17.66s | TPS:   7422.3\n",
      "Loop    9/60 | Steps: 1,392,640 | Fitness:  344.44 | Critic Loss:   0.4614 | Actor Losses: -32.8422 | Alpha Loss: -0.0000 | Alpha Mean: 0.0068 | Time:  17.67s | TPS:   7419.4\n",
      "Loop   10/60 | Steps: 1,523,712 | Fitness:  514.18 | Critic Loss:   0.4968 | Actor Losses: -32.9372 | Alpha Loss: -0.0001 | Alpha Mean: 0.0078 | Time:  17.69s | TPS:   7410.5\n",
      "Loop   11/60 | Steps: 1,654,784 | Fitness:  468.59 | Critic Loss:   0.6345 | Actor Losses: -33.7826 | Alpha Loss: -0.0000 | Alpha Mean: 0.0104 | Time:  17.60s | TPS:   7445.8\n",
      "Loop   12/60 | Steps: 1,785,856 | Fitness:  587.57 | Critic Loss:   0.8448 | Actor Losses: -35.7259 | Alpha Loss: -0.0001 | Alpha Mean: 0.0136 | Time:  17.78s | TPS:   7370.0\n",
      "Checkpoint at loop 12 - Best fitness: 587.57\n",
      "Loop   13/60 | Steps: 1,916,928 | Fitness:  631.08 | Critic Loss:   0.7815 | Actor Losses: -37.4607 | Alpha Loss: 0.0000 | Alpha Mean: 0.0145 | Time:  17.77s | TPS:   7377.0\n",
      "Loop   14/60 | Steps: 2,048,000 | Fitness:  535.28 | Critic Loss:   0.6842 | Actor Losses: -38.0312 | Alpha Loss: 0.0000 | Alpha Mean: 0.0140 | Time:  17.76s | TPS:   7379.4\n",
      "Loop   15/60 | Steps: 2,179,072 | Fitness:  636.52 | Critic Loss:   0.6671 | Actor Losses: -38.7988 | Alpha Loss: 0.0000 | Alpha Mean: 0.0143 | Time:  18.00s | TPS:   7281.0\n",
      "Loop   16/60 | Steps: 2,310,144 | Fitness:  527.79 | Critic Loss:   0.5602 | Actor Losses: -39.7158 | Alpha Loss: 0.0001 | Alpha Mean: 0.0118 | Time:  17.79s | TPS:   7368.8\n",
      "Loop   17/60 | Steps: 2,441,216 | Fitness:  535.59 | Critic Loss:   0.4894 | Actor Losses: -40.1248 | Alpha Loss: 0.0000 | Alpha Mean: 0.0102 | Time:  17.68s | TPS:   7414.0\n",
      "Loop   18/60 | Steps: 2,572,288 | Fitness:  538.12 | Critic Loss:   0.4203 | Actor Losses: -40.7372 | Alpha Loss: 0.0000 | Alpha Mean: 0.0086 | Time:  17.77s | TPS:   7376.7\n",
      "Checkpoint at loop 18 - Best fitness: 636.52\n",
      "Loop   19/60 | Steps: 2,703,360 | Fitness:  584.30 | Critic Loss:   0.3771 | Actor Losses: -41.2752 | Alpha Loss: 0.0000 | Alpha Mean: 0.0077 | Time:  17.74s | TPS:   7388.7\n",
      "Loop   20/60 | Steps: 2,834,432 | Fitness:  602.56 | Critic Loss:   0.3448 | Actor Losses: -41.8810 | Alpha Loss: 0.0000 | Alpha Mean: 0.0079 | Time:  17.72s | TPS:   7395.1\n",
      "Loop   21/60 | Steps: 2,965,504 | Fitness:  597.14 | Critic Loss:   0.2702 | Actor Losses: -42.3274 | Alpha Loss: 0.0000 | Alpha Mean: 0.0064 | Time:  17.74s | TPS:   7387.8\n",
      "Loop   22/60 | Steps: 3,096,576 | Fitness:  582.12 | Critic Loss:   0.2792 | Actor Losses: -42.3604 | Alpha Loss: -0.0000 | Alpha Mean: 0.0073 | Time:  17.70s | TPS:   7405.1\n",
      "Loop   23/60 | Steps: 3,227,648 | Fitness:  557.24 | Critic Loss:   0.2742 | Actor Losses: -42.3146 | Alpha Loss: 0.0000 | Alpha Mean: 0.0071 | Time:  17.72s | TPS:   7398.5\n",
      "Loop   24/60 | Steps: 3,358,720 | Fitness:  588.17 | Critic Loss:   0.2787 | Actor Losses: -42.3551 | Alpha Loss: -0.0000 | Alpha Mean: 0.0077 | Time:  17.86s | TPS:   7337.6\n",
      "Checkpoint at loop 24 - Best fitness: 636.52\n",
      "Loop   25/60 | Steps: 3,489,792 | Fitness:  606.02 | Critic Loss:   0.2736 | Actor Losses: -42.6317 | Alpha Loss: -0.0000 | Alpha Mean: 0.0082 | Time:  17.75s | TPS:   7383.0\n",
      "Loop   26/60 | Steps: 3,620,864 | Fitness:  584.36 | Critic Loss:   0.2773 | Actor Losses: -42.9524 | Alpha Loss: 0.0000 | Alpha Mean: 0.0088 | Time:  17.73s | TPS:   7391.2\n",
      "Loop   27/60 | Steps: 3,751,936 | Fitness:  601.21 | Critic Loss:   0.2765 | Actor Losses: -43.0957 | Alpha Loss: -0.0000 | Alpha Mean: 0.0087 | Time:  17.92s | TPS:   7315.5\n",
      "Loop   28/60 | Steps: 3,883,008 | Fitness:  670.38 | Critic Loss:   0.2922 | Actor Losses: -43.0913 | Alpha Loss: 0.0000 | Alpha Mean: 0.0079 | Time:  17.80s | TPS:   7363.8\n",
      "Loop   29/60 | Steps: 4,014,080 | Fitness:  606.35 | Critic Loss:   0.2716 | Actor Losses: -43.1666 | Alpha Loss: -0.0000 | Alpha Mean: 0.0086 | Time:  17.71s | TPS:   7401.3\n",
      "Loop   30/60 | Steps: 4,145,152 | Fitness:  639.71 | Critic Loss:   0.2344 | Actor Losses: -43.4727 | Alpha Loss: 0.0000 | Alpha Mean: 0.0086 | Time:  17.76s | TPS:   7379.4\n",
      "Checkpoint at loop 30 - Best fitness: 670.38\n",
      "Loop   31/60 | Steps: 4,276,224 | Fitness:  640.47 | Critic Loss:   0.2020 | Actor Losses: -43.7703 | Alpha Loss: -0.0000 | Alpha Mean: 0.0085 | Time:  17.79s | TPS:   7367.0\n",
      "Loop   32/60 | Steps: 4,407,296 | Fitness:  636.21 | Critic Loss:   0.1777 | Actor Losses: -43.9352 | Alpha Loss: 0.0000 | Alpha Mean: 0.0083 | Time:  17.66s | TPS:   7422.6\n",
      "Loop   33/60 | Steps: 4,538,368 | Fitness:  604.49 | Critic Loss:   0.1715 | Actor Losses: -44.3089 | Alpha Loss: -0.0000 | Alpha Mean: 0.0080 | Time:  17.65s | TPS:   7427.5\n",
      "Loop   34/60 | Steps: 4,669,440 | Fitness:  620.17 | Critic Loss:   0.1712 | Actor Losses: -44.2261 | Alpha Loss: 0.0000 | Alpha Mean: 0.0084 | Time:  17.79s | TPS:   7367.3\n",
      "Loop   35/60 | Steps: 4,800,512 | Fitness:  633.02 | Critic Loss:   0.1615 | Actor Losses: -44.2665 | Alpha Loss: 0.0000 | Alpha Mean: 0.0080 | Time:  17.70s | TPS:   7403.1\n",
      "Loop   36/60 | Steps: 4,931,584 | Fitness:  617.93 | Critic Loss:   0.1566 | Actor Losses: -44.3956 | Alpha Loss: -0.0000 | Alpha Mean: 0.0078 | Time:  17.69s | TPS:   7410.5\n",
      "Checkpoint at loop 36 - Best fitness: 670.38\n",
      "Loop   37/60 | Steps: 5,062,656 | Fitness:  632.00 | Critic Loss:   0.1385 | Actor Losses: -44.5699 | Alpha Loss: -0.0000 | Alpha Mean: 0.0079 | Time:  17.74s | TPS:   7387.3\n",
      "Loop   38/60 | Steps: 5,193,728 | Fitness:  621.65 | Critic Loss:   0.1285 | Actor Losses: -44.8152 | Alpha Loss: -0.0000 | Alpha Mean: 0.0083 | Time:  17.66s | TPS:   7423.0\n",
      "Loop   39/60 | Steps: 5,324,800 | Fitness:  599.16 | Critic Loss:   0.1177 | Actor Losses: -44.8750 | Alpha Loss: 0.0000 | Alpha Mean: 0.0083 | Time:  17.66s | TPS:   7421.5\n",
      "Loop   40/60 | Steps: 5,455,872 | Fitness:  624.52 | Critic Loss:   0.1119 | Actor Losses: -44.8538 | Alpha Loss: -0.0000 | Alpha Mean: 0.0080 | Time:  17.72s | TPS:   7396.9\n",
      "Loop   41/60 | Steps: 5,586,944 | Fitness:  605.72 | Critic Loss:   0.1066 | Actor Losses: -44.9105 | Alpha Loss: 0.0000 | Alpha Mean: 0.0078 | Time:  17.79s | TPS:   7367.3\n",
      "Loop   42/60 | Steps: 5,718,016 | Fitness:  621.43 | Critic Loss:   0.1071 | Actor Losses: -44.9686 | Alpha Loss: -0.0000 | Alpha Mean: 0.0081 | Time:  17.94s | TPS:   7307.7\n",
      "Checkpoint at loop 42 - Best fitness: 670.38\n",
      "Loop   43/60 | Steps: 5,849,088 | Fitness:  636.40 | Critic Loss:   0.1033 | Actor Losses: -44.9321 | Alpha Loss: -0.0000 | Alpha Mean: 0.0086 | Time:  17.78s | TPS:   7372.2\n",
      "Loop   44/60 | Steps: 5,980,160 | Fitness:  601.54 | Critic Loss:   0.0952 | Actor Losses: -44.9804 | Alpha Loss: 0.0000 | Alpha Mean: 0.0079 | Time:  17.72s | TPS:   7397.2\n",
      "Loop   45/60 | Steps: 6,111,232 | Fitness:  629.96 | Critic Loss:   0.0991 | Actor Losses: -44.8975 | Alpha Loss: -0.0000 | Alpha Mean: 0.0089 | Time:  17.87s | TPS:   7334.9\n",
      "Loop   46/60 | Steps: 6,242,304 | Fitness:  628.55 | Critic Loss:   0.1006 | Actor Losses: -44.9310 | Alpha Loss: -0.0000 | Alpha Mean: 0.0091 | Time:  17.80s | TPS:   7362.8\n",
      "Loop   47/60 | Steps: 6,373,376 | Fitness:  650.60 | Critic Loss:   0.0985 | Actor Losses: -45.0215 | Alpha Loss: 0.0000 | Alpha Mean: 0.0089 | Time:  17.75s | TPS:   7383.1\n",
      "Loop   48/60 | Steps: 6,504,448 | Fitness:  647.87 | Critic Loss:   0.1005 | Actor Losses: -45.0414 | Alpha Loss: -0.0000 | Alpha Mean: 0.0088 | Time:  17.99s | TPS:   7286.8\n",
      "Checkpoint at loop 48 - Best fitness: 670.38\n",
      "Loop   49/60 | Steps: 6,635,520 | Fitness:  653.40 | Critic Loss:   0.1096 | Actor Losses: -45.1165 | Alpha Loss: -0.0000 | Alpha Mean: 0.0098 | Time:  17.84s | TPS:   7345.6\n",
      "Loop   50/60 | Steps: 6,766,592 | Fitness:  656.38 | Critic Loss:   0.0996 | Actor Losses: -45.2990 | Alpha Loss: 0.0000 | Alpha Mean: 0.0091 | Time:  17.79s | TPS:   7368.3\n",
      "Loop   51/60 | Steps: 6,897,664 | Fitness:  646.73 | Critic Loss:   0.0872 | Actor Losses: -45.4513 | Alpha Loss: 0.0000 | Alpha Mean: 0.0089 | Time:  17.78s | TPS:   7373.7\n",
      "Loop   52/60 | Steps: 7,028,736 | Fitness:  647.21 | Critic Loss:   0.0805 | Actor Losses: -45.5861 | Alpha Loss: 0.0000 | Alpha Mean: 0.0086 | Time:  17.85s | TPS:   7342.6\n",
      "Loop   53/60 | Steps: 7,159,808 | Fitness:  642.73 | Critic Loss:   0.0785 | Actor Losses: -45.6474 | Alpha Loss: 0.0000 | Alpha Mean: 0.0085 | Time:  18.05s | TPS:   7262.2\n",
      "Loop   54/60 | Steps: 7,290,880 | Fitness:  651.43 | Critic Loss:   0.0773 | Actor Losses: -45.6762 | Alpha Loss: 0.0000 | Alpha Mean: 0.0081 | Time:  17.76s | TPS:   7381.8\n",
      "Checkpoint at loop 54 - Best fitness: 670.38\n",
      "Loop   55/60 | Steps: 7,421,952 | Fitness:  665.64 | Critic Loss:   0.0782 | Actor Losses: -45.7187 | Alpha Loss: -0.0000 | Alpha Mean: 0.0082 | Time:  17.83s | TPS:   7351.2\n",
      "Loop   56/60 | Steps: 7,553,024 | Fitness:  663.37 | Critic Loss:   0.0745 | Actor Losses: -45.8506 | Alpha Loss: 0.0000 | Alpha Mean: 0.0082 | Time:  17.73s | TPS:   7392.4\n",
      "Loop   57/60 | Steps: 7,684,096 | Fitness:  649.35 | Critic Loss:   0.0727 | Actor Losses: -46.0251 | Alpha Loss: -0.0000 | Alpha Mean: 0.0083 | Time:  17.79s | TPS:   7367.4\n",
      "Loop   58/60 | Steps: 7,815,168 | Fitness:  653.91 | Critic Loss:   0.0752 | Actor Losses: -45.9983 | Alpha Loss: 0.0000 | Alpha Mean: 0.0088 | Time:  17.77s | TPS:   7377.6\n",
      "Loop   59/60 | Steps: 7,946,240 | Fitness:  657.28 | Critic Loss:   0.0740 | Actor Losses: -45.9375 | Alpha Loss: 0.0000 | Alpha Mean: 0.0086 | Time:  17.90s | TPS:   7323.0\n",
      "\n",
      "Training completed!\n",
      "Total time: 1087.25s\n",
      "Best fitness achieved: 670.38\n",
      "Final training steps: 2334720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tin/anaconda3/envs/mix-me/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:70: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/best_return</td><td>▁▁▂▂▂▂▂▅▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>evaluation/mean_return</td><td>▁▁▂▂▂▂▂▅▇▇▇▅▆▆▇▆▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>evaluation/return_std</td><td>▂▁▂▂▁▂▁▂▁█▄▁▂▂▂▁▂▁▁▄▁▁▂▂▂▂▂▁▂▁▂▁▂▂▃▂▂▂▂▁</td></tr><tr><td>final/best_return</td><td>▁</td></tr><tr><td>final/final_return</td><td>▁</td></tr><tr><td>final/total_time</td><td>▁</td></tr><tr><td>losses/actor_loss_mean</td><td>▇▆██▇▇▇▇▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/agent_0_actor_loss</td><td>▇▆██▇▇▇▇▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/agent_1_actor_loss</td><td>▇▆██▇▇▇▇▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/alpha_loss_mean</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/critic_loss</td><td>█▇▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>performance/loop_time</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>performance/timesteps_per_second</td><td>▁▄██████████████████████████████████████</td></tr><tr><td>performance/total_time</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>temperature/alpha</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training/loop</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training/replay_buffer_size</td><td>▁▂▄▆████████████████████████████████████</td></tr><tr><td>training/timesteps</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training/training_steps</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/best_return</td><td>670.38428</td></tr><tr><td>evaluation/mean_return</td><td>657.28333</td></tr><tr><td>evaluation/return_std</td><td>1.60872</td></tr><tr><td>final/best_return</td><td>670.38428</td></tr><tr><td>final/final_return</td><td>657.28333</td></tr><tr><td>final/total_time</td><td>1087.25413</td></tr><tr><td>losses/actor_loss_mean</td><td>-45.93747</td></tr><tr><td>losses/agent_0_actor_loss</td><td>-45.93767</td></tr><tr><td>losses/agent_1_actor_loss</td><td>-45.93726</td></tr><tr><td>losses/alpha_loss_mean</td><td>0.0</td></tr><tr><td>losses/critic_loss</td><td>0.07397</td></tr><tr><td>performance/loop_time</td><td>17.89859</td></tr><tr><td>performance/timesteps_per_second</td><td>7323.0351</td></tr><tr><td>performance/total_time</td><td>1087.25413</td></tr><tr><td>temperature/alpha</td><td>0.00864</td></tr><tr><td>training/loop</td><td>59</td></tr><tr><td>training/replay_buffer_size</td><td>1000000</td></tr><tr><td>training/timesteps</td><td>7946240</td></tr><tr><td>training/training_steps</td><td>2334720</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MASAC_walker2d_uni_20251027_193525</strong> at: <a href='https://wandb.ai/therealtin-uit/masac-multiagent-rl/runs/a5xjcdvw' target=\"_blank\">https://wandb.ai/therealtin-uit/masac-multiagent-rl/runs/a5xjcdvw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251027_193527-a5xjcdvw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import wandb\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Initialize wandb with proper run naming\n",
    "def init_wandb_logging():\n",
    "    \"\"\"Initialize wandb with descriptive run name\"\"\"\n",
    "    run_name = f\"MASAC_{env_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"masac-multiagent-rl\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            # Environment config\n",
    "            \"env_name\": env_name,\n",
    "            \"episode_length\": episode_length,\n",
    "            \"num_agents\": num_agents,\n",
    "            \"parameter_sharing\": parameter_sharing,\n",
    "            \"emitter_type\": emitter_type,\n",
    "            \"homogenisation_method\": homogenisation_method,\n",
    "            \n",
    "            # Training config\n",
    "            \"num_timesteps\": num_timesteps,\n",
    "            \"env_batch_size\": env_batch_size,\n",
    "            \"warmstart_steps\": warmstart_steps,\n",
    "            \"grad_updates_per_step\": grad_updates_per_step,\n",
    "            \"log_period\": log_period,\n",
    "            \"num_evals\": num_evals,\n",
    "            \n",
    "            # MASAC hyperparameters\n",
    "            \"batch_size\": batch_size,\n",
    "            \"policy_learning_rate\": policy_learning_rate,\n",
    "            \"critic_learning_rate\": critic_learning_rate,\n",
    "            \"alpha_learning_rate\": alpha_learning_rate,\n",
    "            \"discount\": discount,\n",
    "            \"tau\": tau,\n",
    "            \"alpha_init\": alpha_init,\n",
    "            \"fix_alpha\": fix_alpha,\n",
    "            \"reward_scaling\": reward_scaling,\n",
    "            \"replay_buffer_size\": replay_buffer_size,\n",
    "            \"normalize_observations\": normalize_observations,\n",
    "            \"max_grad_norm\": max_grad_norm,\n",
    "            \"target_entropy_scale\": target_entropy_scale,\n",
    "            \"policy_delay\": policy_delay,\n",
    "            \n",
    "            # Network architecture\n",
    "            \"policy_hidden_layer_sizes\": policy_hidden_layer_sizes,\n",
    "            \"critic_hidden_layer_sizes\": critic_hidden_layer_sizes,\n",
    "            \n",
    "            # Other\n",
    "            \"seed\": seed,\n",
    "        },\n",
    "        tags=[\"masac\", \"multiagent\", env_name.split(\"_\")[0]]\n",
    "    )\n",
    "\n",
    "def run_training_loop_with_logging_v2(training_state, replay_buffer, env_states):\n",
    "    \"\"\"Complete training loop with wandb logging and error handling - version with explicit parameters\"\"\"\n",
    "    \n",
    "    # Initialize wandb\n",
    "    init_wandb_logging()\n",
    "    \n",
    "    try:\n",
    "        # Calculate training parameters\n",
    "        num_iters = num_timesteps // env_batch_size\n",
    "        num_loops = num_iters // log_period\n",
    "        \n",
    "        print(f\"Training Configuration:\")\n",
    "        print(f\"  Total timesteps: {num_timesteps:,}\")\n",
    "        print(f\"  Env batch size: {env_batch_size}\")\n",
    "        print(f\"  Total iterations: {num_iters:,}\")\n",
    "        print(f\"  Log period: {log_period}\")\n",
    "        print(f\"  Number of training loops: {num_loops}\")\n",
    "        print(f\"  Warmstart steps: {warmstart_steps:,}\")\n",
    "        \n",
    "        # Initialize random key for evaluation\n",
    "        random_key_local = jax.random.PRNGKey(seed + 1000)  # Different seed for eval\n",
    "        \n",
    "        # Training metrics tracking\n",
    "        start_time = time.time()\n",
    "        best_fitness = float('-inf')\n",
    "        \n",
    "        for i in range(num_loops):\n",
    "            loop_start_time = time.time()\n",
    "            \n",
    "            # Training step\n",
    "            (env_states, training_state, replay_buffer), metrics = jax.lax.scan(\n",
    "                step_and_update,\n",
    "                (env_states, training_state, replay_buffer),\n",
    "                (),\n",
    "                length=log_period\n",
    "            )\n",
    "            metrics = jax.tree_util.tree_map(\n",
    "                lambda x: x.flatten(),\n",
    "                metrics\n",
    "            )\n",
    "\n",
    "            # Evaluation\n",
    "            random_key_local, subkey = jax.random.split(random_key_local)\n",
    "            keys = jax.random.split(subkey, num=num_evals)\n",
    "            reset_states = reset_fn(keys)\n",
    "            \n",
    "            true_return, true_returns = masac_agent.eval_policy_fn(\n",
    "                training_state,\n",
    "                reset_states,\n",
    "                play_eval_step_fn,\n",
    "            )\n",
    "            \n",
    "            # Extract metrics\n",
    "            actor_losses = metrics['actor_losses']\n",
    "            critic_loss = jnp.mean(metrics['critic_loss'], axis=0)\n",
    "            alpha_loss = metrics['alpha_losses']\n",
    "            alpha = metrics['alphas']\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            current_timesteps = warmstart_steps + (i + 1) * env_batch_size * log_period\n",
    "            loop_time = time.time() - loop_start_time\n",
    "            total_time = time.time() - start_time\n",
    "            timesteps_per_second = (env_batch_size * log_period) / loop_time\n",
    "            \n",
    "            # Update best fitness\n",
    "            if true_return > best_fitness:\n",
    "                best_fitness = true_return\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                \"training/timesteps\": current_timesteps,\n",
    "                \"training/loop\": i,\n",
    "                \"evaluation/mean_return\": true_return,\n",
    "                \"evaluation/best_return\": best_fitness,\n",
    "                \"evaluation/return_std\": jnp.std(true_returns),\n",
    "                \"losses/critic_loss\": critic_loss,\n",
    "                \"losses/actor_loss_mean\": jnp.mean(jnp.array(actor_losses), axis=(0, 1)),\n",
    "                \"losses/alpha_loss_mean\": jnp.mean(jnp.array(alpha_loss)),\n",
    "                \"temperature/alpha\": jnp.mean(jnp.array(alpha)),\n",
    "                \"performance/timesteps_per_second\": timesteps_per_second,\n",
    "                \"performance/loop_time\": loop_time,\n",
    "                \"performance/total_time\": total_time,\n",
    "                \"training/replay_buffer_size\": replay_buffer.current_size,\n",
    "                \"training/training_steps\": training_state.steps,\n",
    "                # \"agent/actor_std_mean\": jnp.mean(jnp.array([jnp.exp(p['params']['log_std']) for p in training_state.policy_params]))\n",
    "            })\n",
    "            \n",
    "            # Log individual agent losses and alphas\n",
    "            for agent_idx, (actor_loss, p) in enumerate(zip(actor_losses, training_state.policy_params)):\n",
    "                wandb.log({\n",
    "                    f\"losses/agent_{agent_idx}_actor_loss\": jnp.mean(actor_loss),\n",
    "                    # f\"agent/agent_{agent_idx}_std\": jnp.mean(jnp.array(jnp.exp(p[\"params\"][\"log_std\"]))).item(),\n",
    "                    # f\"temperature/agent_{agent_idx}_alpha_loss\": jnp.mean(alpha_loss),\n",
    "                    # f\"temperature/agent_{agent_idx}_alpha\": jnp.mean(alpha),\n",
    "                })\n",
    "            \n",
    "            # Console output\n",
    "            print(f\"Loop {i:4d}/{num_loops} | \"\n",
    "                  f\"Steps: {current_timesteps:8,} | \"\n",
    "                  f\"Fitness: {true_return:7.2f} | \"\n",
    "                  f\"Critic Loss: {critic_loss:8.4f} | \"\n",
    "                  f\"Actor Losses: {jnp.mean(jnp.array(actor_losses), axis=(0, 1)):.4f} | \"\n",
    "                  f\"Alpha Loss: {jnp.mean(jnp.array(alpha_loss)):.4f} | \"\n",
    "                  f\"Alpha Mean: {jnp.mean(jnp.array(alpha)):.4f} | \"\n",
    "                  f\"Time: {loop_time:6.2f}s | \"\n",
    "                  f\"TPS: {timesteps_per_second:8.1f}\")\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if i % (num_loops // 10) == 0 and i > 0:\n",
    "                print(f\"Checkpoint at loop {i} - Best fitness: {best_fitness:.2f}\")\n",
    "        \n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Total time: {total_time:.2f}s\")\n",
    "        print(f\"Best fitness achieved: {best_fitness:.2f}\")\n",
    "        print(f\"Final training steps: {training_state.steps}\")\n",
    "        \n",
    "        # Final logging\n",
    "        wandb.log({\n",
    "            \"final/best_return\": best_fitness,\n",
    "            \"final/total_time\": total_time,\n",
    "            \"final/final_return\": true_return,\n",
    "        })\n",
    "        \n",
    "        return training_state, replay_buffer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed with error: {e}\")\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        wandb.finish()\n",
    "\n",
    "# Run the training loop\n",
    "final_training_state, final_replay_buffer = run_training_loop_with_logging_v2(\n",
    "    training_state, replay_buffer, env_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9b715",
   "metadata": {},
   "source": [
    "## Multicritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edefd7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mix-me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
