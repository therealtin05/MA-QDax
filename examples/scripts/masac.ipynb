{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d03126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 13:21:53.302768: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.9.86). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from brax.envs import State as EnvState\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Tuple, List\n",
    "\n",
    "from qdax import environments\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs\n",
    "from qdax.core.neuroevolution.buffers.buffer import (\n",
    "    QDTransition,\n",
    "    ReplayBuffer,\n",
    "    Transition,\n",
    ")\n",
    "from qdax.core.neuroevolution.sac_td3_utils import warmstart_buffer, generate_unroll, do_iteration_fn\n",
    "from qdax.core.neuroevolution.buffers.buffer import ReplayBuffer, Transition\n",
    "from qdax.core.neuroevolution.mdp_utils import TrainingState\n",
    "from qdax.custom_types import Metrics\n",
    "from qdax.custom_types import (\n",
    "    Action,\n",
    "    Descriptor,\n",
    "    Mask,\n",
    "    Metrics,\n",
    "    Observation,\n",
    "    Params,\n",
    "    Reward,\n",
    "    RNGKey,\n",
    ")\n",
    "\n",
    "# Multiagent imports\n",
    "from qdax.environments.multi_agent_wrappers import MultiAgentBraxWrapper\n",
    "# jax.config.update(\"jax_log_compiles\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245481c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "env_name = 'ant_uni'#@param['ant_uni', 'hopper_uni', 'walker_uni', 'halfcheetah_uni', 'humanoid_uni', 'ant_omni', 'humanoid_omni']\n",
    "parameter_sharing=False\n",
    "emitter_type=\"mix\"\n",
    "homogenisation_method=\"concat\"\n",
    "episode_length = 1000 #@param {type:\"integer\"}\n",
    "num_timesteps = 7_864_320 #@param {type:\"integer\"}\n",
    "# num_timesteps = 1_000_000 #@param {type:\"integer\"}\n",
    "seed = 0 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (64, 64) #@param {type:\"raw\"}\n",
    "policy_learning_rate = 3e-4\n",
    "critic_learning_rate = 3e-4\n",
    "alpha_learning_rate = 3e-4\n",
    "num_init_cvt_samples = 50000 #@param {type:\"integer\"}\n",
    "num_centroids = 1024 #@param {type:\"integer\"}\n",
    "min_bd = 0. #@param {type:\"number\"}\n",
    "max_bd = 1.0 #@param {type:\"number\"}\n",
    "warmstart_steps=8192*10\n",
    "num_evals=20    \n",
    "log_period=1024 \n",
    "\n",
    "# SAC params\n",
    "env_batch_size = 128 #@param {type:\"number\"}\n",
    "batch_size=512\n",
    "grad_updates_per_step=0.3 #@param {type:\"number\"}\n",
    "replay_buffer_size = 1000000 #@param {type:\"number\"}\n",
    "critic_hidden_layer_sizes = (256, 256) #@param {type:\"raw\"}\n",
    "discount = 0.99 #@param {type:\"number\"}\n",
    "reward_scaling = 1.0 #@param {type:\"number\"}s\n",
    "tau = 0.005 #@param {type:\"number\"}  # SAC uses tau instead of soft_tau_update\n",
    "alpha_init=1.0 #@param {type:\"number\"}\n",
    "fix_alpha=False #@param {type:\"boolean\"}\n",
    "normalize_observations=False #@param {type:\"boolean\"}\n",
    "max_grad_norm=1_000.0 #@param {type:\"number\"}\n",
    "policy_delay = 3\n",
    "target_entropy_scale = 0.5\n",
    "independent_std = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdax.core.neuroevolution.networks.masac_networks import make_masac_networks\n",
    "from qdax.core.neuroevolution.losses.masac_loss import masac_critic_loss_fn, masac_policy_loss_fn, masac_alpha_loss_fn\n",
    "from qdax.baselines.masac import MASAC, MASacConfig, MASacTrainingState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8d5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Warmstart related functions\n",
    "import functools\n",
    "\n",
    "# Define the random step function\n",
    "def warmstart_play_step_fn(\n",
    "    env_state: EnvState,\n",
    "    random_key: RNGKey,\n",
    "    env: MultiAgentBraxWrapper,\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "\n",
    "    action_sizes = env.get_action_sizes()\n",
    "\n",
    "    keys = jax.random.split(subkey, len(action_sizes))\n",
    "\n",
    "    actions = {\n",
    "        agent_idx: jax.random.uniform(agent_key, (size,), minval=-1, maxval=1)\n",
    "        for (agent_idx, size), agent_key in zip(action_sizes.items(), keys)\n",
    "    }\n",
    "\n",
    "    flatten_actions = jnp.concatenate([a for a in actions.values()])\n",
    "\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=next_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        actions=flatten_actions,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "    )\n",
    "\n",
    "    return next_state, random_key, transition\n",
    "\n",
    "def generate_unroll_warmstart(\n",
    "    random_key: RNGKey,\n",
    "    env_state: EnvState,\n",
    "    env: MultiAgentBraxWrapper,\n",
    "    warmstart_play_step_fn: Callable[\n",
    "        [EnvState, RNGKey, MultiAgentBraxWrapper],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    "    warmstart_steps: int,\n",
    ") -> Tuple[EnvState, Transition]:\n",
    "    \"\"\"Pre-populates the buffer with transitions. Returns the warmstarted buffer\n",
    "    and the new state of the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def _scan_play_step_fn(\n",
    "        carry: Tuple[EnvState, RNGKey], unused_arg: Any\n",
    "    ) -> Tuple[Tuple[EnvState, RNGKey], Transition]:\n",
    "        env_state, random_key, transitions = warmstart_play_step_fn(*carry, env)\n",
    "        return (env_state, random_key), transitions\n",
    "\n",
    "    (env_state, random_key), transitions = jax.lax.scan(\n",
    "        _scan_play_step_fn,\n",
    "        (env_state, random_key),\n",
    "        (),\n",
    "        length=warmstart_steps\n",
    "    )\n",
    "\n",
    "    return env_state, transitions\n",
    "\n",
    "@functools.partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"env\", \"warmstart_play_step_fn\", \"warmstart_steps\", \"env_batch_size\")\n",
    ")\n",
    "def warmstart_buffer(\n",
    "    env: MultiAgentBraxWrapper,\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    training_state: MASacTrainingState,\n",
    "    warmstart_play_step_fn: Callable[\n",
    "        [EnvState, RNGKey, MultiAgentBraxWrapper],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    "    warmstart_steps: int,\n",
    "    env_batch_size: int,\n",
    "):\n",
    "    \n",
    "    generate_unroll = functools.partial(\n",
    "        generate_unroll_warmstart,\n",
    "        env = env,\n",
    "        warmstart_play_step_fn=warmstart_play_step_fn,\n",
    "        warmstart_steps=warmstart_steps//env_batch_size\n",
    "    )\n",
    "\n",
    "    generate_unroll_vmap = jax.vmap(\n",
    "        generate_unroll,\n",
    "        in_axes=(0, 0)\n",
    "    )\n",
    "\n",
    "    random_key, subkey = jax.random.split(training_state.random_key)\n",
    "    keys = jax.random.split(subkey, env_batch_size)\n",
    "\n",
    "    training_state = training_state.replace(\n",
    "        random_key=random_key\n",
    "    )\n",
    "\n",
    "    reset_fn = jax.vmap(env.reset)\n",
    "\n",
    "    env_states = reset_fn(keys)\n",
    "\n",
    "    random_key, subkey = jax.random.split(training_state.random_key)\n",
    "\n",
    "    training_state = training_state.replace(\n",
    "        random_key=random_key\n",
    "    )\n",
    "\n",
    "    keys = jax.random.split(subkey, env_batch_size)\n",
    "    env_states, transitions = generate_unroll_vmap(keys, env_states)\n",
    "\n",
    "    replay_buffer = replay_buffer.insert(transitions)\n",
    "    \n",
    "    return replay_buffer, training_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097fca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env_name = env_name.split(\"_\")[0]\n",
    "env = environments.create(env_name, episode_length=episode_length)\n",
    "env = MultiAgentBraxWrapper(\n",
    "    env,\n",
    "    env_name=base_env_name,\n",
    "    parameter_sharing=False,\n",
    "    emitter_type=emitter_type,\n",
    "    homogenisation_method=homogenisation_method\n",
    ")\n",
    "\n",
    "random_key = jax.random.PRNGKey(seed)\n",
    "num_agents = len(env.get_action_sizes())\n",
    "\n",
    "# Make sure to pass the correct config parameters\n",
    "masac_config = MASacConfig(\n",
    "    num_agents=len(env.get_action_sizes()),\n",
    "    episode_length=episode_length,\n",
    "    batch_size=batch_size,\n",
    "    tau=tau,  # SAC uses tau instead of soft_tau_update\n",
    "    normalize_observations=normalize_observations,\n",
    "    policy_learning_rate=policy_learning_rate,  # SAC typically uses same LR for all components\n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    alpha_learning_rate=alpha_learning_rate,\n",
    "    alpha_init=alpha_init,\n",
    "    discount=discount,\n",
    "    reward_scaling=reward_scaling,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_sizes,\n",
    "    policy_hidden_layer_size=policy_hidden_layer_sizes,\n",
    "    fix_alpha=fix_alpha,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    target_entropy_scale=target_entropy_scale,\n",
    "    independent_std=independent_std,\n",
    "    policy_delay=policy_delay\n",
    ")\n",
    "\n",
    "masac_agent = MASAC(config=masac_config, action_sizes=env.get_action_sizes())\n",
    "\n",
    "training_state = masac_agent.init(\n",
    "    random_key=random_key, \n",
    "    action_sizes_each_agent=env.get_action_sizes(),\n",
    "    observation_size_raw=env.observation_size,\n",
    "    observation_sizes_each_agent=env.get_obs_sizes()\n",
    ")\n",
    "\n",
    "reset_fn = jax.vmap(env.reset)\n",
    "\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, env_batch_size)\n",
    "\n",
    "env_states = reset_fn(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d599a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init replay buffer\n",
    "dummy_transition = QDTransition.init_dummy(\n",
    "    observation_dim=env.observation_size, \n",
    "    action_dim=env.action_size, \n",
    "    descriptor_dim=env.behavior_descriptor_length\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer.init(buffer_size=replay_buffer_size, transition=dummy_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56aade8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer, training_state = warmstart_buffer(\n",
    "    env=env,\n",
    "    replay_buffer=replay_buffer,\n",
    "    training_state=training_state,\n",
    "    warmstart_play_step_fn=warmstart_play_step_fn,\n",
    "    warmstart_steps=warmstart_steps,\n",
    "    env_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2343dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_step_fn = functools.partial(\n",
    "    masac_agent.play_qd_step_fn,\n",
    "    env=env,\n",
    "    deterministic=False\n",
    ")\n",
    "\n",
    "# Create the scan_update function\n",
    "@functools.partial(jax.jit, static_argnames=(\"unflatten_obs_fn\", \"unflatten_actions_fn\"))\n",
    "def scan_update(\n",
    "    carry: Tuple[MASacTrainingState, ReplayBuffer],\n",
    "    unused: Any,\n",
    "    unflatten_obs_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    "    unflatten_actions_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]]\n",
    ") -> Tuple[Tuple[MASacTrainingState, ReplayBuffer], Metrics]:\n",
    "    \"\"\"Single update step for the scan operation\"\"\"\n",
    "    training_state, replay_buffer = carry\n",
    "    \n",
    "    # Perform one update step\n",
    "    new_training_state, new_replay_buffer, metrics = masac_agent.update(\n",
    "        training_state, \n",
    "        replay_buffer,\n",
    "        unflatten_obs_fn,\n",
    "        unflatten_actions_fn\n",
    "    )\n",
    "    \n",
    "    return (new_training_state, new_replay_buffer), metrics\n",
    "\n",
    "# Now create the clean single_step_and_update function\n",
    "@functools.partial(jax.jit, static_argnames=(\"play_step_fn\", \"unflatten_obs_fn\", \"unflatten_actions_fn\", \"num_updates\"))\n",
    "def single_step_and_update(\n",
    "    carry: [EnvState, MASacTrainingState, ReplayBuffer],\n",
    "    _,\n",
    "    play_step_fn: Callable[\n",
    "        [EnvState, MASacTrainingState],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            MASacTrainingState,\n",
    "            QDTransition,\n",
    "        ],\n",
    "    ],\n",
    "    unflatten_obs_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    "    unflatten_actions_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    "    num_updates: int\n",
    ") -> Tuple[Tuple[EnvState, MASacTrainingState, ReplayBuffer], Metrics]:\n",
    "    \"\"\"Performs one environment step followed by multiple gradient updates\"\"\"\n",
    "    \n",
    "    # Vectorized environment step\n",
    "    play_step_fn_vmap = jax.vmap(\n",
    "        play_step_fn, \n",
    "        in_axes=(0, None), \n",
    "        out_axes=(0, None, 0)\n",
    "    )\n",
    "\n",
    "    env_states, training_state, replay_buffer = carry\n",
    "\n",
    "    env_states, training_state, transitions = play_step_fn_vmap(env_states, training_state)\n",
    "\n",
    "    # Insert transitions into replay buffer\n",
    "    replay_buffer = replay_buffer.insert(transitions)\n",
    "    \n",
    "    # Create partial function for scan_update with fixed unflatten functions\n",
    "    scan_update_partial = functools.partial(\n",
    "        scan_update, \n",
    "        unflatten_obs_fn=unflatten_obs_fn, \n",
    "        unflatten_actions_fn=unflatten_actions_fn,\n",
    "    )\n",
    "\n",
    "    # Perform multiple gradient updates\n",
    "    (training_state, replay_buffer), metrics = jax.lax.scan(\n",
    "        scan_update_partial,\n",
    "        (training_state, replay_buffer),\n",
    "        (),\n",
    "        length=num_updates\n",
    "    )\n",
    "\n",
    "    return (env_states, training_state, replay_buffer), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a72c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten_obs_fn(global_obs: jnp.ndarray, env: MultiAgentBraxWrapper) -> dict[int, jnp.ndarray]:\n",
    "    agent_obs = {}\n",
    "    for agent_idx, obs_indices in env.agent_obs_mapping.items():\n",
    "        agent_obs[agent_idx] = global_obs[obs_indices]\n",
    "    return agent_obs\n",
    "\n",
    "def unflatten_actions_fn(flatten_action: jnp.ndarray, env: MultiAgentBraxWrapper) -> dict[int, jax.Array]:\n",
    "    \"\"\"Because the actions in the form of Dict[int, jnp.array] is flatten by \n",
    "    flatten_actions = jnp.concatenate([a for a in actions.values()]) so we do this way\n",
    "    \"\"\"\n",
    "    actions = {}\n",
    "    start = 0\n",
    "    for agent_idx, size in env.get_action_sizes().items():\n",
    "        end = start + size\n",
    "        actions[agent_idx] = flatten_action[start:end]\n",
    "        start = end\n",
    "    return actions\n",
    "\n",
    "unflatten_obs_fn = functools.partial(\n",
    "    unflatten_obs_fn,\n",
    "    env=env\n",
    ")\n",
    "\n",
    "unflatten_actions_fn = functools.partial(\n",
    "    unflatten_actions_fn,\n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd9dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_and_update = functools.partial(\n",
    "    single_step_and_update,\n",
    "    play_step_fn=play_step_fn,\n",
    "    unflatten_obs_fn=unflatten_obs_fn,\n",
    "    unflatten_actions_fn=unflatten_actions_fn,\n",
    "    num_updates=int(grad_updates_per_step * env_batch_size)\n",
    ")\n",
    "\n",
    "\n",
    "play_eval_step_fn = functools.partial(\n",
    "    masac_agent.play_qd_step_fn,\n",
    "    env=env,\n",
    "    deterministic=True\n",
    "    # deterministic=False\n",
    ")\n",
    "\n",
    "play_eval_step_fn = jax.vmap(\n",
    "    play_eval_step_fn,\n",
    "    in_axes=(0, None),\n",
    "    out_axes=(0, None, 0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbc3b6",
   "metadata": {},
   "source": [
    "## Single critic and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f3457d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_state.policy_params[0][\"params\"][\"log_std\"])\n",
    "# print(jnp.exp(training_state.policy_params[0][\"params\"][\"log_std\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22faef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for agent_idx, p in enumerate(training_state.policy_params):\n",
    "#     print(f\"agent/agent_{agent_idx}_std {jnp.mean(jnp.array(jnp.exp(p['params']['log_std'])))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b13ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tin/anaconda3/envs/mix-me/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtherealtin\u001b[0m (\u001b[33mtherealtin-uit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/tin/anaconda3/envs/mix-me/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:70: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tin/Desktop/HaiDang/RL/Mix-ME/MA-QDax/examples/scripts/wandb/run-20250810_132348-2hpmpq3j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/therealtin-uit/masac-multiagent-rl/runs/2hpmpq3j' target=\"_blank\">MASAC_ant_uni_20250810_132346</a></strong> to <a href='https://wandb.ai/therealtin-uit/masac-multiagent-rl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/therealtin-uit/masac-multiagent-rl' target=\"_blank\">https://wandb.ai/therealtin-uit/masac-multiagent-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/therealtin-uit/masac-multiagent-rl/runs/2hpmpq3j' target=\"_blank\">https://wandb.ai/therealtin-uit/masac-multiagent-rl/runs/2hpmpq3j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Total timesteps: 7,864,320\n",
      "  Env batch size: 128\n",
      "  Total iterations: 61,440\n",
      "  Log period: 1024\n",
      "  Number of training loops: 60\n",
      "  Warmstart steps: 81,920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tin/Desktop/HaiDang/RL/Mix-ME/MA-QDax/qdax/core/neuroevolution/mdp_utils.py:83: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(mask_episodes, transition)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop    0/60 | Steps:  212,992 | Fitness:  823.54 | Critic Loss:   3.2353 | Actor Losses: -25.2471 | Alpha Loss: 0.8711 | Alpha Mean: 0.2980 | Time:  71.93s | TPS:   1822.2\n",
      "Loop    1/60 | Steps:  344,064 | Fitness:  949.73 | Critic Loss:   1.1843 | Actor Losses: -15.3855 | Alpha Loss: 0.0126 | Alpha Mean: 0.0305 | Time:  55.79s | TPS:   2349.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import wandb\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Initialize wandb with proper run naming\n",
    "def init_wandb_logging():\n",
    "    \"\"\"Initialize wandb with descriptive run name\"\"\"\n",
    "    run_name = f\"MASAC_{env_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"masac-multiagent-rl\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            # Environment config\n",
    "            \"env_name\": env_name,\n",
    "            \"episode_length\": episode_length,\n",
    "            \"num_agents\": num_agents,\n",
    "            \"parameter_sharing\": parameter_sharing,\n",
    "            \"emitter_type\": emitter_type,\n",
    "            \"homogenisation_method\": homogenisation_method,\n",
    "            \n",
    "            # Training config\n",
    "            \"num_timesteps\": num_timesteps,\n",
    "            \"env_batch_size\": env_batch_size,\n",
    "            \"warmstart_steps\": warmstart_steps,\n",
    "            \"grad_updates_per_step\": grad_updates_per_step,\n",
    "            \"log_period\": log_period,\n",
    "            \"num_evals\": num_evals,\n",
    "            \n",
    "            # MASAC hyperparameters\n",
    "            \"batch_size\": batch_size,\n",
    "            \"policy_learning_rate\": policy_learning_rate,\n",
    "            \"critic_learning_rate\": critic_learning_rate,\n",
    "            \"alpha_learning_rate\": alpha_learning_rate,\n",
    "            \"discount\": discount,\n",
    "            \"tau\": tau,\n",
    "            \"alpha_init\": alpha_init,\n",
    "            \"fix_alpha\": fix_alpha,\n",
    "            \"reward_scaling\": reward_scaling,\n",
    "            \"replay_buffer_size\": replay_buffer_size,\n",
    "            \"normalize_observations\": normalize_observations,\n",
    "            \"max_grad_norm\": max_grad_norm,\n",
    "            \"target_entropy_scale\": target_entropy_scale,\n",
    "            \"policy_delay\": policy_delay,\n",
    "            \n",
    "            # Network architecture\n",
    "            \"policy_hidden_layer_sizes\": policy_hidden_layer_sizes,\n",
    "            \"critic_hidden_layer_sizes\": critic_hidden_layer_sizes,\n",
    "            \n",
    "            # Other\n",
    "            \"seed\": seed,\n",
    "        },\n",
    "        tags=[\"masac\", \"multiagent\", env_name.split(\"_\")[0]]\n",
    "    )\n",
    "\n",
    "def run_training_loop_with_logging_v2(training_state, replay_buffer, env_states):\n",
    "    \"\"\"Complete training loop with wandb logging and error handling - version with explicit parameters\"\"\"\n",
    "    \n",
    "    # Initialize wandb\n",
    "    init_wandb_logging()\n",
    "    \n",
    "    try:\n",
    "        # Calculate training parameters\n",
    "        num_iters = num_timesteps // env_batch_size\n",
    "        num_loops = num_iters // log_period\n",
    "        \n",
    "        print(f\"Training Configuration:\")\n",
    "        print(f\"  Total timesteps: {num_timesteps:,}\")\n",
    "        print(f\"  Env batch size: {env_batch_size}\")\n",
    "        print(f\"  Total iterations: {num_iters:,}\")\n",
    "        print(f\"  Log period: {log_period}\")\n",
    "        print(f\"  Number of training loops: {num_loops}\")\n",
    "        print(f\"  Warmstart steps: {warmstart_steps:,}\")\n",
    "        \n",
    "        # Initialize random key for evaluation\n",
    "        random_key_local = jax.random.PRNGKey(seed + 1000)  # Different seed for eval\n",
    "        \n",
    "        # Training metrics tracking\n",
    "        start_time = time.time()\n",
    "        best_fitness = float('-inf')\n",
    "        \n",
    "        for i in range(num_loops):\n",
    "            loop_start_time = time.time()\n",
    "            \n",
    "            # Training step\n",
    "            (env_states, training_state, replay_buffer), metrics = jax.lax.scan(\n",
    "                step_and_update,\n",
    "                (env_states, training_state, replay_buffer),\n",
    "                (),\n",
    "                length=log_period\n",
    "            )\n",
    "            metrics = jax.tree_util.tree_map(\n",
    "                lambda x: x.flatten(),\n",
    "                metrics\n",
    "            )\n",
    "\n",
    "            # Evaluation\n",
    "            random_key_local, subkey = jax.random.split(random_key_local)\n",
    "            keys = jax.random.split(subkey, num=num_evals)\n",
    "            reset_states = reset_fn(keys)\n",
    "            \n",
    "            true_return, true_returns = masac_agent.eval_policy_fn(\n",
    "                training_state,\n",
    "                reset_states,\n",
    "                play_eval_step_fn,\n",
    "            )\n",
    "            \n",
    "            # Extract metrics\n",
    "            actor_losses = metrics['actor_losses']\n",
    "            critic_loss = jnp.mean(metrics['critic_loss'], axis=0)\n",
    "            alpha_loss = metrics['alpha_loss']\n",
    "            alpha = metrics['alpha']\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            current_timesteps = warmstart_steps + (i + 1) * env_batch_size * log_period\n",
    "            loop_time = time.time() - loop_start_time\n",
    "            total_time = time.time() - start_time\n",
    "            timesteps_per_second = (env_batch_size * log_period) / loop_time\n",
    "            \n",
    "            # Update best fitness\n",
    "            if true_return > best_fitness:\n",
    "                best_fitness = true_return\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                \"training/timesteps\": current_timesteps,\n",
    "                \"training/loop\": i,\n",
    "                \"evaluation/mean_return\": true_return,\n",
    "                \"evaluation/best_return\": best_fitness,\n",
    "                \"evaluation/return_std\": jnp.std(true_returns),\n",
    "                \"losses/critic_loss\": critic_loss,\n",
    "                \"losses/actor_loss_mean\": jnp.mean(jnp.array(actor_losses), axis=(0, 1)),\n",
    "                \"losses/alpha_loss_mean\": jnp.mean(alpha_loss),\n",
    "                \"temperature/alpha\": jnp.mean(alpha),\n",
    "                \"performance/timesteps_per_second\": timesteps_per_second,\n",
    "                \"performance/loop_time\": loop_time,\n",
    "                \"performance/total_time\": total_time,\n",
    "                \"training/replay_buffer_size\": replay_buffer.current_size,\n",
    "                \"training/training_steps\": training_state.steps,\n",
    "                # \"agent/actor_std_mean\": jnp.mean(jnp.array([jnp.exp(p['params']['log_std']) for p in training_state.policy_params]))\n",
    "            })\n",
    "            \n",
    "            # Log individual agent losses and alphas\n",
    "            for agent_idx, (actor_loss, p) in enumerate(zip(actor_losses, training_state.policy_params)):\n",
    "                wandb.log({\n",
    "                    f\"losses/agent_{agent_idx}_actor_loss\": jnp.mean(actor_loss),\n",
    "                    # f\"agent/agent_{agent_idx}_std\": jnp.mean(jnp.array(jnp.exp(p[\"params\"][\"log_std\"]))).item(),\n",
    "                    # f\"temperature/agent_{agent_idx}_alpha_loss\": jnp.mean(alpha_loss),\n",
    "                    # f\"temperature/agent_{agent_idx}_alpha\": jnp.mean(alpha),\n",
    "                })\n",
    "            \n",
    "            # Console output\n",
    "            print(f\"Loop {i:4d}/{num_loops} | \"\n",
    "                  f\"Steps: {current_timesteps:8,} | \"\n",
    "                  f\"Fitness: {true_return:7.2f} | \"\n",
    "                  f\"Critic Loss: {critic_loss:8.4f} | \"\n",
    "                  f\"Actor Losses: {jnp.mean(jnp.array(actor_losses), axis=(0, 1)):.4f} | \"\n",
    "                  f\"Alpha Loss: {jnp.mean(alpha_loss):.4f} | \"\n",
    "                  f\"Alpha Mean: {jnp.mean(alpha):.4f} | \"\n",
    "                  f\"Time: {loop_time:6.2f}s | \"\n",
    "                  f\"TPS: {timesteps_per_second:8.1f}\")\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if i % (num_loops // 10) == 0 and i > 0:\n",
    "                print(f\"Checkpoint at loop {i} - Best fitness: {best_fitness:.2f}\")\n",
    "        \n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Total time: {total_time:.2f}s\")\n",
    "        print(f\"Best fitness achieved: {best_fitness:.2f}\")\n",
    "        print(f\"Final training steps: {training_state.steps}\")\n",
    "        \n",
    "        # Final logging\n",
    "        wandb.log({\n",
    "            \"final/best_return\": best_fitness,\n",
    "            \"final/total_time\": total_time,\n",
    "            \"final/final_return\": true_return,\n",
    "        })\n",
    "        \n",
    "        return training_state, replay_buffer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed with error: {e}\")\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        wandb.finish()\n",
    "\n",
    "# Run the training loop\n",
    "final_training_state, final_replay_buffer = run_training_loop_with_logging_v2(\n",
    "    training_state, replay_buffer, env_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9b715",
   "metadata": {},
   "source": [
    "## Multicritic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mix-me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
