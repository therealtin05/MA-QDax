{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea3706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 21:29:00.603055: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.9.86). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from brax.envs import State as EnvState\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "from qdax import environments\n",
    "from qdax.tasks.brax_envs import reset_based_scoring_function_brax_envs\n",
    "from qdax.baselines.td3 import TD3Config, TD3, TD3TrainingState\n",
    "from qdax.core.neuroevolution.buffers.buffer import (\n",
    "    QDTransition,\n",
    "    ReplayBuffer,\n",
    "    Transition,\n",
    ")\n",
    "from qdax.core.neuroevolution.sac_td3_utils import warmstart_buffer, generate_unroll, do_iteration_fn\n",
    "from qdax.core.neuroevolution.buffers.buffer import ReplayBuffer, Transition\n",
    "from qdax.core.neuroevolution.mdp_utils import TrainingState\n",
    "from qdax.custom_types import Metrics\n",
    "from qdax.custom_types import (\n",
    "    Action,\n",
    "    Descriptor,\n",
    "    Mask,\n",
    "    Metrics,\n",
    "    Observation,\n",
    "    Params,\n",
    "    Reward,\n",
    "    RNGKey,\n",
    ")\n",
    "\n",
    "# Multiagent shiet\n",
    "from qdax.environments.multi_agent_wrappers import MultiAgentBraxWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8598672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title QD Training Definitions Fields\n",
    "#@markdown ---\n",
    "env_name = 'hopper_uni'#@param['ant_uni', 'hopper_uni', 'walker_uni', 'halfcheetah_uni', 'humanoid_uni', 'ant_omni', 'humanoid_omni']\n",
    "parameter_sharing=False\n",
    "emitter_type=\"mix\"\n",
    "homogenisation_method=\"concat\"\n",
    "episode_length = 1000 #@param {type:\"integer\"}\n",
    "num_timesteps = 7_864_320 #@param {type:\"integer\"}\n",
    "seed = 1 #@param {type:\"integer\"}\n",
    "policy_hidden_layer_sizes = (64, 64) #@param {type:\"raw\"}\n",
    "policy_learning_rate = 3e-4\n",
    "num_init_cvt_samples = 50000 #@param {type:\"integer\"}\n",
    "num_centroids = 1024 #@param {type:\"integer\"}\n",
    "min_bd = 0. #@param {type:\"number\"}\n",
    "max_bd = 1.0 #@param {type:\"number\"}\n",
    "warmstart_steps=8192*10\n",
    "num_evals=20\n",
    "log_period=1024\n",
    "# proportion_mutation_ga = 0.5 #@param {type:\"number\"}\n",
    "\n",
    "# TD3 params\n",
    "env_batch_size = 128 #@param {type:\"number\"}\n",
    "batch_size=256\n",
    "expl_noise = 0.1\n",
    "policy_noise = 0.2\n",
    "noise_clip = 0.5\n",
    "grad_updates_per_step=0.5 #@param {type:\"number\"}\n",
    "replay_buffer_size = 1_000_000 #@param {type:\"number\"}\n",
    "critic_hidden_layer_sizes = (256, 256) #@param {type:\"raw\"}\n",
    "critic_learning_rate = 3e-4\n",
    "discount = 0.99 #@param {type:\"number\"}\n",
    "reward_scaling = 1.0 #@param {type:\"number\"}\n",
    "soft_tau_update = 0.005 #@param {type:\"number\"}\n",
    "policy_delay = 2 #@param {type:\"number\"}\n",
    "alpha_init=1.0\n",
    "fix_alpha=False\n",
    "max_grad_norm = 1000.0\n",
    "use_layer_norm=True\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2520be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdax.core.neuroevolution.networks.matd3_networks import make_matd3_networks\n",
    "from qdax.core.neuroevolution.losses.matd3_loss import matd3_critic_loss_fn, matd3_policy_loss_fn\n",
    "from qdax.baselines.matd3 import MATD3, MATD3Config, MATD3TrainingState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832d82c",
   "metadata": {},
   "source": [
    "## Warmstart related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4558126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Define the fonction random\n",
    "def warmstart_play_step_fn(\n",
    "    env_state: EnvState,\n",
    "    random_key: RNGKey,\n",
    "    env: MultiAgentBraxWrapper,\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "\n",
    "    action_sizes = env.get_action_sizes()\n",
    "\n",
    "    keys = jax.random.split(subkey, len(action_sizes))\n",
    "\n",
    "    actions = {\n",
    "        agent_idx: jax.random.uniform(agent_key, (size,), minval=-1, maxval=1)\n",
    "        for (agent_idx, size), agent_key in zip(action_sizes.items(), keys)\n",
    "    }\n",
    "\n",
    "    flatten_actions = jnp.concatenate([a for a in actions.values()])\n",
    "\n",
    "    state_desc = env_state.info[\"state_descriptor\"]\n",
    "    next_state = env.step(env_state, actions)\n",
    "\n",
    "    transition = QDTransition(\n",
    "        obs=next_state.obs,\n",
    "        next_obs=next_state.obs,\n",
    "        rewards=next_state.reward,\n",
    "        dones=next_state.done,\n",
    "        actions=flatten_actions,\n",
    "        truncations=next_state.info[\"truncation\"],\n",
    "        state_desc=state_desc,\n",
    "        next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "    )\n",
    "\n",
    "    return next_state, random_key, transition\n",
    "\n",
    "def generate_unroll_warmstart(\n",
    "    random_key: RNGKey,\n",
    "    env_state: EnvState,\n",
    "    env: MultiAgentBraxWrapper,\n",
    "    warmstart_play_step_fn: Callable[\n",
    "        [EnvState, RNGKey, MultiAgentBraxWrapper],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    "    warmstart_steps: int,\n",
    ") -> Tuple[EnvState, Transition]:\n",
    "    \"\"\"Pre-populates the buffer with transitions. Returns the warmstarted buffer\n",
    "    and the new state of the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def _scan_play_step_fn(\n",
    "        carry: Tuple[EnvState, RNGKey], unused_arg: Any\n",
    "    ) -> Tuple[Tuple[EnvState, RNGKey], Transition]:\n",
    "        env_state, random_key, transitions = warmstart_play_step_fn(*carry, env)\n",
    "        return (env_state, random_key), transitions\n",
    "\n",
    "    (env_state, random_key), transitions = jax.lax.scan(\n",
    "        _scan_play_step_fn,\n",
    "        (env_state, random_key),\n",
    "        (),\n",
    "        length=warmstart_steps\n",
    "    )\n",
    "\n",
    "    return env_state, transitions\n",
    "\n",
    "@functools.partial(\n",
    "        jax.jit,\n",
    "        static_argnames=(\"env\", \"warmstart_play_step_fn\", \"warmstart_steps\", \"env_batch_size\")\n",
    ")\n",
    "def warmstart_buffer(\n",
    "    env: MultiAgentBraxWrapper,\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    training_state: MATD3TrainingState,\n",
    "    warmstart_play_step_fn: Callable[\n",
    "        [EnvState, RNGKey, MultiAgentBraxWrapper],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    "    warmstart_steps: int,\n",
    "    env_batch_size: int,\n",
    "):\n",
    "    \n",
    "\n",
    "    generate_unroll = functools.partial(\n",
    "        generate_unroll_warmstart,\n",
    "        env = env,\n",
    "        warmstart_play_step_fn=warmstart_play_step_fn,\n",
    "        warmstart_steps=warmstart_steps//env_batch_size\n",
    "    )\n",
    "\n",
    "    generate_unroll_vmap = jax.vmap(\n",
    "        generate_unroll,\n",
    "        in_axes=(0, 0)\n",
    "    )\n",
    "\n",
    "    random_key, subkey = jax.random.split(training_state.random_key)\n",
    "    keys = jax.random.split(subkey, env_batch_size)\n",
    "\n",
    "\n",
    "    training_state = training_state.replace(\n",
    "        random_key=random_key\n",
    "    )\n",
    "\n",
    "    reset_fn = jax.vmap(env.reset)\n",
    "\n",
    "    env_states = reset_fn(keys)\n",
    "\n",
    "    random_key, subkey = jax.random.split(training_state.random_key)\n",
    "\n",
    "    training_state = training_state.replace(\n",
    "        random_key=random_key\n",
    "    )\n",
    "\n",
    "    keys = jax.random.split(subkey, env_batch_size)\n",
    "    env_states, transitions = generate_unroll_vmap(keys, env_states)\n",
    "\n",
    "    # jax.debug.print(\"obs shape {obs}\", obs=transitions.obs.shape)\n",
    "\n",
    "    replay_buffer = replay_buffer.insert(transitions)\n",
    "    \n",
    "    return replay_buffer, training_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1efe1f",
   "metadata": {},
   "source": [
    "## Prepare env and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c6605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1, 2: 1}\n",
      "{0: 8, 1: 9, 2: 8}\n"
     ]
    }
   ],
   "source": [
    "base_env_name = env_name.split(\"_\")[0]\n",
    "env = environments.create(env_name, episode_length=episode_length)\n",
    "env = MultiAgentBraxWrapper(\n",
    "    env,\n",
    "    env_name=base_env_name,\n",
    "    parameter_sharing=False,\n",
    "    emitter_type=emitter_type,\n",
    "    homogenisation_method=homogenisation_method\n",
    ")\n",
    "\n",
    "print(env.get_action_sizes())\n",
    "print(env.get_obs_sizes())\n",
    "\n",
    "random_key = jax.random.PRNGKey(seed)\n",
    "num_agents = len(env.get_action_sizes())\n",
    "\n",
    "# Make sure to pass the correct config parameters\n",
    "matd3_config = MATD3Config(\n",
    "    num_agents=len(env.get_action_sizes()),\n",
    "    episode_length=episode_length,\n",
    "    batch_size=batch_size,\n",
    "    policy_delay=policy_delay,\n",
    "    soft_tau_update=soft_tau_update,\n",
    "    expl_noise=expl_noise,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_sizes,  # Fixed parameter name\n",
    "    policy_hidden_layer_size=policy_hidden_layer_sizes,  # Fixed parameter name\n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    policy_learning_rate=policy_learning_rate,\n",
    "    discount=discount,\n",
    "    noise_clip=noise_clip,\n",
    "    policy_noise=policy_noise,\n",
    "    reward_scaling=reward_scaling,\n",
    "    max_grad_norm=max_grad_norm\n",
    ")\n",
    "\n",
    "matd3_agent = MATD3(config=matd3_config, action_sizes=env.get_action_sizes())\n",
    "\n",
    "training_state = matd3_agent.init(random_key=random_key, \n",
    "                                  action_sizes_each_agent=env.get_action_sizes(),\n",
    "                                  observation_size_raw=env.observation_size,\n",
    "                                  observation_sizes_each_agent=env.get_obs_sizes())\n",
    "\n",
    "# env_state = env.reset(random_key)\n",
    "\n",
    "reset_fn = jax.vmap(env.reset)\n",
    "\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "keys = jax.random.split(subkey, env_batch_size)\n",
    "\n",
    "env_states = reset_fn(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614e27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init replay buffer\n",
    "dummmy_transition = QDTransition.init_dummy(observation_dim=env.observation_size, action_dim=env.action_size, descriptor_dim=env.behavior_descriptor_length)\n",
    "\n",
    "replay_buffer = ReplayBuffer.init(buffer_size=replay_buffer_size,  transition=dummmy_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b63997",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer, training_state = warmstart_buffer(\n",
    "    env=env,\n",
    "    replay_buffer=replay_buffer,\n",
    "    training_state=training_state,\n",
    "    warmstart_play_step_fn = warmstart_play_step_fn,\n",
    "    warmstart_steps=warmstart_steps,\n",
    "    env_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb122f",
   "metadata": {},
   "source": [
    "## Policy | Env | replay_buffer vmap/scan interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21ce08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_step_fn = functools.partial(\n",
    "    matd3_agent.play_qd_step_fn,\n",
    "    env=env,\n",
    "    deterministic=False\n",
    ")\n",
    "\n",
    "# Create the scan_update function\n",
    "@functools.partial(jax.jit, static_argnames=(\"unflatten_obs_fn\", \"unflatten_actions_fn\"))\n",
    "def scan_update(\n",
    "    carry: Tuple[MATD3TrainingState, ReplayBuffer],\n",
    "    unused: Any,\n",
    "    unflatten_obs_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    "    unflatten_actions_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    ") -> Tuple[Tuple[MATD3TrainingState, ReplayBuffer], Metrics]:\n",
    "    \"\"\"Single update step for the scan operation\"\"\"\n",
    "    training_state, replay_buffer = carry\n",
    "    \n",
    "    # Perform one update step\n",
    "    new_training_state, new_replay_buffer, metrics = matd3_agent.update(\n",
    "        training_state, \n",
    "        replay_buffer,\n",
    "        unflatten_obs_fn,\n",
    "        unflatten_actions_fn,\n",
    "    )\n",
    "    \n",
    "    return (new_training_state, new_replay_buffer), metrics\n",
    "\n",
    "# Now create the clean single_step_and_update function\n",
    "@functools.partial(jax.jit, static_argnames=(\"play_step_fn\", \"unflatten_obs_fn\", \"unflatten_actions_fn\", \"num_updates\"))\n",
    "def single_step_and_update(\n",
    "    carry: [EnvState, MATD3TrainingState, ReplayBuffer],\n",
    "    _,\n",
    "    # env_states: EnvState,\n",
    "    # replay_buffer: ReplayBuffer,\n",
    "    # training_state: MATD3TrainingState,\n",
    "    play_step_fn: Callable[\n",
    "        [EnvState, MATD3TrainingState],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            MATD3TrainingState,\n",
    "            QDTransition,\n",
    "        ],\n",
    "    ],\n",
    "    unflatten_obs_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    "    unflatten_actions_fn: Callable[[jnp.ndarray], dict[int, jnp.ndarray]],\n",
    "    num_updates: int\n",
    ") -> Tuple[Tuple[EnvState, MATD3TrainingState, ReplayBuffer], Metrics]:\n",
    "    \"\"\"Performs one environment step followed by multiple gradient updates\"\"\"\n",
    "    \n",
    "    # Vectorized environment step\n",
    "    play_step_fn_vmap = jax.vmap(\n",
    "        play_step_fn, \n",
    "        in_axes=(0, None), \n",
    "        out_axes=(0, None, 0)\n",
    "    )\n",
    "\n",
    "    env_states,training_state, replay_buffer = carry\n",
    "\n",
    "    env_states, training_state, transitions = play_step_fn_vmap(env_states, training_state)\n",
    "\n",
    "    # Insert transitions into replay buffer\n",
    "    replay_buffer = replay_buffer.insert(transitions)\n",
    "    \n",
    "    # Create partial function for scan_update with fixed unflatten_obs_fn\n",
    "    scan_update_partial = functools.partial(scan_update, unflatten_obs_fn=unflatten_obs_fn, unflatten_actions_fn=unflatten_actions_fn)\n",
    "\n",
    "    # Perform multiple gradient updates\n",
    "    (training_state, replay_buffer), metrics = jax.lax.scan(\n",
    "        scan_update_partial,\n",
    "        (training_state, replay_buffer),\n",
    "        (),\n",
    "        length=num_updates\n",
    "    )\n",
    "\n",
    "    return (env_states, training_state, replay_buffer), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750efd09",
   "metadata": {},
   "source": [
    "## Functions to change flatten obs, actions to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3cc2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten_obs_fn(global_obs: jnp.ndarray, env:MultiAgentBraxWrapper) -> dict[int, jnp.ndarray]:\n",
    "    agent_obs = {}\n",
    "    for agent_idx, obs_indices in env.agent_obs_mapping.items():\n",
    "            agent_obs[agent_idx] = global_obs[obs_indices]\n",
    "    return agent_obs\n",
    "\n",
    "def unflatten_actions_fn(flatten_action: jnp.ndarray, env:MultiAgentBraxWrapper) -> dict[int, jax.Array]:\n",
    "    \"\"\"Because the actions in the form of Dict[int, jnp.array] is flatten by \n",
    "    flatten_actions = jnp.concatenate([a for a in actions.values()]) so we do this way\n",
    "    \"\"\"\n",
    "\n",
    "    actions = {}\n",
    "    start = 0\n",
    "    for agent_idx, size in env.get_action_sizes().items():\n",
    "        end = start + size\n",
    "        actions[agent_idx] = flatten_action[start:end]\n",
    "        start = end\n",
    "    return actions\n",
    "\n",
    "unflatten_obs_fn = functools.partial(\n",
    "    unflatten_obs_fn,\n",
    "    env=env\n",
    ")\n",
    "\n",
    "unflatten_actions_fn = functools.partial(\n",
    "    unflatten_actions_fn,\n",
    "    env=env\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b43fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_and_update = functools.partial(\n",
    "    single_step_and_update,\n",
    "    play_step_fn=play_step_fn,\n",
    "    unflatten_obs_fn=unflatten_obs_fn,\n",
    "    unflatten_actions_fn=unflatten_actions_fn,\n",
    "    num_updates=int(grad_updates_per_step * env_batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33be00f",
   "metadata": {},
   "source": [
    "## Functions related to evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a2bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_eval_step_fn = functools.partial(\n",
    "    matd3_agent.play_qd_step_fn,\n",
    "    env=env,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "play_eval_step_fn = jax.vmap(\n",
    "    play_eval_step_fn,\n",
    "    in_axes=(0, None),\n",
    "    out_axes=(0, None, 0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847cdc9c",
   "metadata": {},
   "source": [
    "## Training/logging loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759602d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tin/anaconda3/envs/mix-me/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtherealtin\u001b[0m (\u001b[33mtherealtin-uit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/tin/anaconda3/envs/mix-me/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:82: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73a70505c754be19d83e662930e773e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666886651664754, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tin/Desktop/HaiDang/RL/Mix-ME/MA-QDax/examples/scripts/wandb/run-20251113_212915-aq78t3dm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/therealtin-uit/matd3-multiagent-rl/runs/aq78t3dm' target=\"_blank\">MATD3_hopper_uni_20251113_212913</a></strong> to <a href='https://wandb.ai/therealtin-uit/matd3-multiagent-rl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/therealtin-uit/matd3-multiagent-rl' target=\"_blank\">https://wandb.ai/therealtin-uit/matd3-multiagent-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/therealtin-uit/matd3-multiagent-rl/runs/aq78t3dm' target=\"_blank\">https://wandb.ai/therealtin-uit/matd3-multiagent-rl/runs/aq78t3dm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Total timesteps: 7,864,320\n",
      "  Env batch size: 128\n",
      "  Total iterations: 61,440\n",
      "  Log period: 1024\n",
      "  Number of training loops: 60\n",
      "  Warmstart steps: 81,920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tin/Desktop/HaiDang/RL/Mix-ME/MA-QDax/qdax/core/neuroevolution/mdp_utils.py:83: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(mask_episodes, transition)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop    0/60 | Steps:  212,992 | Fitness:  315.15 | Critic Loss:  42.6010 | Actor Losses: -84.24068450927734 | Time:  48.28s | TPS:   2715.0\n",
      "Loop    1/60 | Steps:  344,064 | Fitness:  307.51 | Critic Loss:  45.7200 | Actor Losses: -121.82445526123047 | Time:  34.64s | TPS:   3783.7\n",
      "Loop    2/60 | Steps:  475,136 | Fitness:  321.32 | Critic Loss:  26.5849 | Actor Losses: -116.05723571777344 | Time:  29.06s | TPS:   4511.1\n",
      "Loop    3/60 | Steps:  606,208 | Fitness:  331.50 | Critic Loss:  19.6352 | Actor Losses: -120.12629699707031 | Time:  28.53s | TPS:   4594.9\n",
      "Loop    4/60 | Steps:  737,280 | Fitness:  343.35 | Critic Loss:  16.2964 | Actor Losses: -121.6728515625 | Time:  28.51s | TPS:   4597.4\n",
      "Loop    5/60 | Steps:  868,352 | Fitness:  356.42 | Critic Loss:  16.2936 | Actor Losses: -127.5977783203125 | Time:  28.54s | TPS:   4593.3\n",
      "Loop    6/60 | Steps:  999,424 | Fitness:  361.87 | Critic Loss:  14.1059 | Actor Losses: -129.04486083984375 | Time:  28.65s | TPS:   4575.2\n",
      "Checkpoint at loop 6 - Best fitness: 361.87\n",
      "Loop    7/60 | Steps: 1,130,496 | Fitness:  352.44 | Critic Loss:  10.3542 | Actor Losses: -128.8804931640625 | Time:  28.64s | TPS:   4576.7\n",
      "Loop    8/60 | Steps: 1,261,568 | Fitness:  350.40 | Critic Loss:   3.9883 | Actor Losses: -132.34844970703125 | Time:  28.54s | TPS:   4592.5\n",
      "Loop    9/60 | Steps: 1,392,640 | Fitness:  356.92 | Critic Loss:   2.9593 | Actor Losses: -133.66102600097656 | Time:  28.52s | TPS:   4595.6\n",
      "Loop   10/60 | Steps: 1,523,712 | Fitness:  351.87 | Critic Loss:   2.8471 | Actor Losses: -136.01007080078125 | Time:  28.72s | TPS:   4564.1\n",
      "Loop   11/60 | Steps: 1,654,784 | Fitness:  348.51 | Critic Loss:   2.9148 | Actor Losses: -136.02694702148438 | Time:  28.71s | TPS:   4565.1\n",
      "Loop   12/60 | Steps: 1,785,856 | Fitness:  349.26 | Critic Loss:   3.0457 | Actor Losses: -135.25782775878906 | Time:  28.77s | TPS:   4556.6\n",
      "Checkpoint at loop 12 - Best fitness: 361.87\n",
      "Loop   13/60 | Steps: 1,916,928 | Fitness:  340.98 | Critic Loss:   3.4175 | Actor Losses: -132.76963806152344 | Time:  28.57s | TPS:   4587.7\n",
      "Loop   14/60 | Steps: 2,048,000 | Fitness:  317.63 | Critic Loss:   3.3380 | Actor Losses: -130.44593811035156 | Time:  28.56s | TPS:   4588.6\n",
      "Loop   15/60 | Steps: 2,179,072 | Fitness:  360.50 | Critic Loss:   3.1330 | Actor Losses: -131.00140380859375 | Time:  28.57s | TPS:   4588.5\n",
      "Loop   16/60 | Steps: 2,310,144 | Fitness:  349.13 | Critic Loss:   2.9337 | Actor Losses: -130.85816955566406 | Time:  28.49s | TPS:   4600.5\n",
      "Loop   17/60 | Steps: 2,441,216 | Fitness:  338.89 | Critic Loss:   3.0852 | Actor Losses: -130.43475341796875 | Time:  28.51s | TPS:   4598.1\n",
      "Loop   18/60 | Steps: 2,572,288 | Fitness:  343.56 | Critic Loss:   2.9222 | Actor Losses: -130.7313690185547 | Time:  28.65s | TPS:   4575.3\n",
      "Checkpoint at loop 18 - Best fitness: 361.87\n",
      "Loop   19/60 | Steps: 2,703,360 | Fitness:  337.62 | Critic Loss:   2.6254 | Actor Losses: -131.18621826171875 | Time:  28.69s | TPS:   4569.1\n",
      "Loop   20/60 | Steps: 2,834,432 | Fitness:  365.55 | Critic Loss:   2.6481 | Actor Losses: -131.5096893310547 | Time:  28.86s | TPS:   4541.7\n",
      "Loop   21/60 | Steps: 2,965,504 | Fitness:  349.73 | Critic Loss:   2.5792 | Actor Losses: -130.76817321777344 | Time:  28.57s | TPS:   4587.8\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize wandb with proper run naming\n",
    "def init_wandb_logging():\n",
    "    \"\"\"Initialize wandb with descriptive run name\"\"\"\n",
    "    run_name = f\"MATD3_{env_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"matd3-multiagent-rl\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            # Environment config\n",
    "            \"env_name\": env_name,\n",
    "            \"episode_length\": episode_length,\n",
    "            \"num_agents\": num_agents,\n",
    "            \"parameter_sharing\": parameter_sharing,\n",
    "            \"emitter_type\": emitter_type,\n",
    "            \"homogenisation_method\": homogenisation_method,\n",
    "            \n",
    "            # Training config\n",
    "            \"num_timesteps\": num_timesteps,\n",
    "            \"env_batch_size\": env_batch_size,\n",
    "            \"warmstart_steps\": warmstart_steps,\n",
    "            \"grad_updates_per_step\": grad_updates_per_step,\n",
    "            \"log_period\": log_period,\n",
    "            \"num_evals\": num_evals,\n",
    "            \n",
    "            # MATD3 hyperparameters\n",
    "            \"batch_size\": batch_size,\n",
    "            \"policy_learning_rate\": policy_learning_rate,\n",
    "            \"critic_learning_rate\": critic_learning_rate,\n",
    "            \"discount\": discount,\n",
    "            \"soft_tau_update\": soft_tau_update,\n",
    "            \"policy_delay\": policy_delay,\n",
    "            \"expl_noise\": expl_noise,\n",
    "            \"noise_clip\": noise_clip,\n",
    "            \"reward_scaling\": reward_scaling,\n",
    "            \"replay_buffer_size\": replay_buffer_size,\n",
    "            \n",
    "            # Network architecture\n",
    "            \"policy_hidden_layer_sizes\": policy_hidden_layer_sizes,\n",
    "            \"critic_hidden_layer_sizes\": critic_hidden_layer_sizes,\n",
    "            \n",
    "            # Other\n",
    "            \"seed\": seed,\n",
    "        },\n",
    "        tags=[\"matd3\", \"multiagent\", env_name.split(\"_\")[0]]\n",
    "    )\n",
    "\n",
    "# Alternative: Pass parameters explicitly to the function\n",
    "def run_training_loop_with_logging_v2(training_state, replay_buffer, env_states):\n",
    "    \"\"\"Complete training loop with wandb logging and error handling - version with explicit parameters\"\"\"\n",
    "    \n",
    "    # Initialize wandb\n",
    "    init_wandb_logging()\n",
    "    \n",
    "    try:\n",
    "        # Calculate training parameters\n",
    "        num_iters = num_timesteps // env_batch_size\n",
    "        num_loops = num_iters // log_period\n",
    "        \n",
    "        print(f\"Training Configuration:\")\n",
    "        print(f\"  Total timesteps: {num_timesteps:,}\")\n",
    "        print(f\"  Env batch size: {env_batch_size}\")\n",
    "        print(f\"  Total iterations: {num_iters:,}\")\n",
    "        print(f\"  Log period: {log_period}\")\n",
    "        print(f\"  Number of training loops: {num_loops}\")\n",
    "        print(f\"  Warmstart steps: {warmstart_steps:,}\")\n",
    "        \n",
    "        # Initialize random key for evaluation\n",
    "        random_key_local = jax.random.PRNGKey(seed + 1000)  # Different seed for eval\n",
    "        \n",
    "        # Training metrics tracking\n",
    "        start_time = time.time()\n",
    "        best_fitness = float('-inf')\n",
    "        \n",
    "        for i in range(num_loops):\n",
    "            loop_start_time = time.time()\n",
    "            \n",
    "            # Training step\n",
    "            (env_states, training_state, replay_buffer), metrics = jax.lax.scan(\n",
    "                step_and_update,\n",
    "                (env_states, training_state, replay_buffer),\n",
    "                (),\n",
    "                length=log_period\n",
    "            )\n",
    "            metrics = jax.tree_util.tree_map(\n",
    "                lambda x: x.flatten(),\n",
    "                metrics\n",
    "            )\n",
    "\n",
    "            # print(metrics['actor_losses'][0].shape, metrics['critic_loss'].shape)\n",
    "            # Evaluation\n",
    "            random_key_local, subkey = jax.random.split(random_key_local)\n",
    "            keys = jax.random.split(subkey, num=num_evals)\n",
    "            reset_states = reset_fn(keys)\n",
    "            \n",
    "            true_return, true_returns = matd3_agent.eval_policy_fn(\n",
    "                training_state,\n",
    "                reset_states,\n",
    "                play_eval_step_fn,\n",
    "            )\n",
    "            \n",
    "            actor_losses = metrics['actor_losses']\n",
    "            \n",
    "            critic_loss = jnp.mean(metrics['critic_loss'], axis=0)\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            current_timesteps = warmstart_steps + (i + 1) * env_batch_size * log_period\n",
    "            loop_time = time.time() - loop_start_time\n",
    "            total_time = time.time() - start_time\n",
    "            timesteps_per_second = (env_batch_size * log_period) / loop_time\n",
    "            \n",
    "            # Update best fitness\n",
    "            if true_return > best_fitness:\n",
    "                best_fitness = true_return\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                \"training/timesteps\": current_timesteps,\n",
    "                \"training/loop\": i,\n",
    "                \"evaluation/mean_return\": true_return,\n",
    "                \"evaluation/best_return\": best_fitness,\n",
    "                \"evaluation/return_std\": jnp.std(true_returns),\n",
    "                \"losses/critic_loss\": critic_loss,\n",
    "                \"losses/actor_loss_mean\": jnp.mean(jnp.array(actor_losses), axis=(0, 1)),\n",
    "\n",
    "                \"performance/timesteps_per_second\": timesteps_per_second,\n",
    "                \"performance/loop_time\": loop_time,\n",
    "                \"performance/total_time\": total_time,\n",
    "                \"training/replay_buffer_size\": replay_buffer.current_size,\n",
    "                \"training/training_steps\": training_state.steps,\n",
    "            })\n",
    "            \n",
    "            # Log individual agent losses\n",
    "            for agent_idx, loss in enumerate(actor_losses):\n",
    "                wandb.log({f\"losses/agent_{agent_idx}_loss\": jnp.mean(loss)})\n",
    "            \n",
    "            # Console output\n",
    "            print(f\"Loop {i:4d}/{num_loops} | \"\n",
    "                  f\"Steps: {current_timesteps:8,} | \"\n",
    "                  f\"Fitness: {true_return:7.2f} | \"\n",
    "                  f\"Critic Loss: {critic_loss:8.4f} | \"\n",
    "                  f\"Actor Losses: {jnp.mean(jnp.array(actor_losses), axis=(0, 1))} | \"\n",
    "                  f\"Time: {loop_time:6.2f}s | \"\n",
    "                  f\"TPS: {timesteps_per_second:8.1f}\")\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if i % (num_loops // 10) == 0 and i > 0:\n",
    "                print(f\"Checkpoint at loop {i} - Best fitness: {best_fitness:.2f}\")\n",
    "        \n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Total time: {total_time:.2f}s\")\n",
    "        print(f\"Best fitness achieved: {best_fitness:.2f}\")\n",
    "        print(f\"Final training steps: {training_state.steps}\")\n",
    "        \n",
    "        # Final logging\n",
    "        wandb.log({\n",
    "            \"final/best_return\": best_fitness,\n",
    "            \"final/total_time\": total_time,\n",
    "            \"final/final_return\": true_return,\n",
    "        })\n",
    "        \n",
    "        return training_state, replay_buffer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed with error: {e}\")\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        wandb.finish()\n",
    "\n",
    "# Run the training loop - use the version that passes parameters explicitly\n",
    "final_training_state, final_replay_buffer = run_training_loop_with_logging_v2(\n",
    "    training_state, replay_buffer, env_states\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mix-me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
